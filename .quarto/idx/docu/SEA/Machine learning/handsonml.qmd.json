{"title":"Hands-On Machine Learning with Scikit-Learn ,Keras and Tensorflow","markdown":{"yaml":{"title":"Hands-On Machine Learning with Scikit-Learn ,Keras and Tensorflow","execute":{"warning":false,"freeze":true,"keep-ipynb":true},"toc":true,"format":{"html":{"html-math-method":"katex","df-print":"paged","code-overflow":"wrap","code-copy":true}},"jupyter":"python3"},"headingText":"Introduction","containsRefs":false,"markdown":"\n\n![Image of the book](IMG_9683.jpg)\n\n\nA pure code based implementation with necessary mathematical formulations included.\n\n### End to End machine Learning\n\n### Performance Measure\n\n<b> <u>Root Mean Square Error </b> </u> $$ RMSE(\\textbf{X},h)= \\sqrt{\\frac{\\sum_{i = 1}^{m}{(h{(x^{(i)})- y ^{(i)}}})^2}{m}}$$ $m$- is the number of instances in the datasets you are measuring the RMSE on <br /> $x^{(i)}$- is the vector of all teh feature values of the $i^{th}$ instance in the dataset<br/> $y^{(i)}$- is its label<br/> $\\textbf{X}$- is the matrix containing all the feature value of all the instances in the dataset. $x^{(i)}$ transpose is the $i^{th}$-row in $\\textbf{X}$ </br> h - is the system's prediction function also called as hypothesis. It is also represented by $\\hat{y}$ where $\\hat{y}^{(i)} = h(x^{(i)})$</br> $RMSE(\\textbf{X},h)$- is the cost function measured on the set of examples using the hypothesis h </br>\n\n<b> <u>Mean absolute Error</b> </u > $$ MAE(\\textbf{X},h)= \\frac{\\sum_{i = 1}^{m}{|h{(x^{(i)})- y ^{(i)}}}|}{m}$$\n\nThe difference between these two performance measure is how the norm is caculated. RSME makes use of $\\mathcal{l}_2$ norm while MAE makes use of $\\mathcal{l}_1$ norm.\n\n#### Download the DATA\n\n```{python}\nimport os \nimport tarfile\nimport urllib\n\nDownload_Root = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\nHousing_Path = os.path.join(\"Data\",\"housing\") # for creating your local directory with DATA as main directory and housign as sub directory\nHousing_URL = Download_Root + \"datasets/housing/housing.tgz\"\n\ndef fetch_housing_data(housing_url = Housing_URL,housing_path = Housing_Path):\n    os.makedirs(housing_path,exist_ok = True)# Will create a directory with name at housing path , exist_ok will check if it already exist , if yes leaving it unaltered\n    tgz_path = os.path.join(housing_path,\"housing.tgz\")\n    urllib.request.urlretrieve(housing_url,tgz_path)\n    housing_tgz = tarfile.open(tgz_path)\n    housing_tgz.extractall(path=housing_path)\n    housing_tgz.close()\n    \nfetch_housing_data() # call the function\n```\n\n```{python}\nimport pandas as pd\n\ndef load_housing_data(housing_path = Housing_Path):\n    csv_path = os.path.join(housing_path,\"housing.csv\")\n    return pd.read_csv(csv_path)\n```\n\n```{python}\nhousing = load_housing_data()\nhousing.head()\n```\n\n```{python}\nhousing.info()\n```\n\n```{python}\nhousing[\"ocean_proximity\"].value_counts()\n```\n\n```{python}\nhousing.describe()\n```\n\n```{python}\n%matplotlib inline \nimport matplotlib.pyplot as plt\nhousing.hist(bins = 50 ,figsize = (20,15))\n```\n\nFew point about the data\n\n-   The median income is capped at 15 for higher median incomes and at 0.5 for lower median incomes. The numbers are denominated in \\$10000 i.e 3 in the chart represents 30,000 USD\n\n-   The housing median age and median house value were also capped. Since median house value is our target variable this is a source of a problem.The prediction might never go beyond the capped limit.\n\n-   The attributes have different scales\n\n-   Many of the histograms are tail-heavy. Important to normalize the data as some machine learning algorithims might fail to detect patterns effectively\n\n#### Creating a test set and train set\n\n```{python}\nimport numpy as np\nnp.random.seed(50)# will always give the same test set and train set \ndef split_train_test(data,test_ratio):\n    shuffled_indices = np.random.permutation(len(data))\n    test_set_size = int(len(data)*test_ratio)\n    test_indices = shuffled_indices[:test_set_size]\n    train_indices = shuffled_indices[test_set_size:]\n    return data.loc[train_indices] ,data.loc[test_indices]\n\ntrain_set , test_set = split_train_test(housing,0.2)\nprint(len(train_set))\nprint(len(test_set))\n```\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"paged","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":true,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"wrap","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"toc-depth":4,"html-math-method":"katex","output-file":"handsonml.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.269","jupyter":"python3","theme":"Darkly","toc-location":"right","title":"Hands-On Machine Learning with Scikit-Learn ,Keras and Tensorflow","code-copy":true},"extensions":{"book":{"multiFile":true}}}}}