{"title":"Statistics","markdown":{"yaml":{"title":"Statistics"},"headingText":"Introduction","containsRefs":false,"markdown":"\n\n\nThis is majorl\n## Standard Definitions\n\n<u>Expected Value</u> $\\mathbb{E}[X]$ is given by\n\n$$\\mathbb{E}[X] = \\int_{-\\infty}^{\\infty}x f(x) dx$$ <u>Variance</u>$Var(X)$ is given by\n\n$$Var(X) = \\mathbb{E}(X^2) - \\mathbb{E}{{(X)}^2}$$ $$Var(X) = \\int_{-\\infty}^{\\infty}(x-\\mathbb{E}[X])^2 f_X(x) dx$$ <u>Higher Moments</u> $\\mathbb{E}(X^n)$ is given by\n\n$$\\mathbb{E}(X^n) = \\int_{-\\infty}^{\\infty}x^n f_X(x) dx $$ <u>Characteristic function(CHF)</u> $\\phi_X(u)$ for $u \\in \\mathbb{R}$ is given by\n\n$$\\phi_X(u) = \\mathbb{E}[e^{iuX}] = \\int_{-\\infty}^{\\infty}e^{iuX}f(x)dx $$\n\n<u>Moment generating function</u>$\\mathcal{M}_X(u)$ is given by $$\\mathcal{M}_X(u) = \\phi_X(-iu)= \\mathbb{E}[e^{uX}] = \\int_{-\\infty}^{\\infty}e^{ux}f(x)dx $$ <u>Cumulant characteristic function</u> $\\zeta_X(u)$ is given by $$\\zeta_X(u) = log\\mathbb{E}[e^{iux}] = log\\phi_X(u)$$\n\n<u>Central moments</u>$ \\mathcal{m}_l$ is given by $\\mathbb{E}[(X-\\mu)^l]$\n\nSkewness $S(x)$ and Kurtosis $K(x)$ are the normalised $3^{rd}$ and $4^{th}$ central moments of a distribution respectively. The normalization factors are $\\sigma^3$ and $\\sigma^4$ respectively where $\\sigma$ is the standard deviation of X.\n\nThe quantity $K(x) - 3$ is called the excess kurtosis since $K(x) = 3$ is the kurtosis for a normal distribution.\n\nLet $\\{x_1,x_2,x_3 ....x_T\\}$ be a random sample of X with T observations <br>\n \n<u>Sample Mean</u>$\\hat\\mu_x$ is given by $$\\frac{\\sum_{t=1}^Tx_t}{T}$$\n<u>Sample Variance</u>$\\hat\\sigma_x$ is given by $$\\frac{\\sum_{t=1}^T(x_t - \\hat\\mu_x)^2}{T-1}$$\n<u>Sample Skewness</u>$\\hat S_x$ is given by $$\\frac{\\sum_{t=1}^T(x_t - \\hat\\mu_x)^3}{(T-1)\\hat\\sigma_x^3}$$\n<u>Sample Kurtosis</u>$\\hat K_x$ is given by $$\\frac{\\sum_{t=1}^T(x_t - \\hat\\mu_x)^4}{(T-1)\\hat\\sigma_x^4}$$\n\n\n\n\n## Univaiate Distributions\n\n\n### Normal Distribution\n\nA random variable $X$ is said to be normally distrbuted if it has a probability density function as follows\n\n$$f_X(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-{\\frac{1}{2}}(\\frac{x-\\mu}{\\sigma})^2}$$\n\nIt is a continous probability distribution\n\n$\\mu$ and $\\sigma$ are the mean and variance of the distribution respectively\n\nThe case where $\\mu =0$ and $\\sigma = 1$ is called standard normal distribution and its PDF is given by $$  f_X(x) =\\frac{1}{\\sqrt{2\\pi}}e^{\\frac{-x^2}{2}}$$\n\n```{python}\nimport numpy as np\nimport math \nimport matplotlib.pyplot as plt\nimport scipy.stats as st\nfrom mpl_toolkits import mplot3d\n\n\ndef plotNormalPDF_CDF_CHF(mu ,sigma):\n    i = complex(0,1)\n    chf = lambda u : np.exp(i*mu*u -(sigma**2)*u*u/2)\n    pdf = lambda x : st.norm.pdf(x,mu,sigma)\n    cdf = lambda x : st.norm.cdf(x,mu,sigma)\n    \n    x = np.linspace(5,15,100)\n    u = np.linspace(0,5,250)\n    print(type(pdf))\n    # figure 1 ,PDF\n    plt.figure(1)\n    plt.plot(x,pdf(x))\n    plt.grid()\n    plt.xlabel('x')\n    plt.ylabel('PDF')\n  \n    # figure 2 ,CDF\n    plt.figure(2)\n    plt.plot(x,cdf(x))\n    plt.grid()\n    plt.xlabel('x')\n    plt.ylabel('CDF')\n  \n    #  figure 3 ,CHF\n  \n    plt.figure(3)\n    ax = plt.axes(projection = '3d')\n    chfV = chf(u)\n  \n    x = np.real(chfV)\n    y = np.imag(chfV)\n    ax.plot3D(u,x,y,'red')\n    ax.view_init(30 ,-120)\n    \nplotNormalPDF_CDF_CHF(10,1)\n  \n```\n\n### Log Normal Distibution\n\nA random Variable $X$ is said to have log normal distibution if $Y = \\ln{X}$ and $Y$ is normally distributed.\n\nThe PDF of log normal distribution is given by\n\n$$f_X(x) = \\frac{1}{x\\sigma\\sqrt{2\\pi}}e^{(-\\frac{(\\ln{x} -\\mu)^2}{2{\\sigma}^2})}$$ where $\\mu$ and $\\sigma$ are the mean and variance of $Y(\\ln X)$ respectively.\n\nHence the mean $\\mu^*$ and variance $\\sigma^*$ of X are as follows\n\n$$\\mu^* = e^{\\mu + \\frac{1}{2}\\sigma^2}$$ $$\\sigma^* = e^{2\\mu + 2\\sigma^2} - e^{2\\mu +\\sigma^2}$$ Important thing to note here is that $x$ can take values in $(0,\\infty)$ only.\n\n## Multivariate Distributions\n\n### Correlation\n\nThe correlation coefficient between two random variables $X$ and $Y$ is defined as $$ \\rho_{x,y} = \\frac{Cov(X,Y)}{\\sqrt{Var(X)Var(Y)}} = \\frac{E[(X-\\mu_x)(Y-\\mu_y)]}{\\sqrt{E(X-\\mu_x)^2E(Y-\\mu_y)^2}}$$\n\nThe sample correlation is given by $$ \\hat\\rho_{x,y} = \\frac{\\sum_{t=1}^{T}(x_t - \\bar{x})(y_t - \\bar{y})}{\\sqrt{\\sum_{t=1}^T(x_t - \\bar{x})\\sum_{t=1}^T(y_t - \\bar{y})}}$$\n\n\n<u>Two-dimensional densities</u>.\\newline\n\n\nThe joint CDF of two random variables ,$X$ and $Y$ ,is the function $F_{X,Y}(.,.):\\mathbb{R}^2 \\rightarrow [0,1]$,which is defined by:\n\n$$ F_{X,Y}(x,y) = \\mathbb{P}[X\\leq{x},Y\\leq{y}]$$\nIf $X$ and $Y$ are continous variables, then the joint PDF of X and Y is a function of \n$$f_{X,Y}(x,y) = \\frac{\\partial^2{F_{X,Y}(x,y)}}{\\partial{x}\\partial{y}} $$\n<u>Bivariate Normal density functions</u>\n\n$X = [X,Y]^T$ and $$X \\sim \\mathcal{N}(\\begin{bmatrix}0\\\\0\\end{bmatrix},\\begin{bmatrix}1 , \\rho \\\\ \\rho ,1\\end{bmatrix}) $$\n\n```{python}\nimport numpy as np\nimport matplotlib.pyplot as plt\n#from matplotlib.mlab import bivariate_normal bivariate_normal seems to be deprecated\n\ndef bivariate_normal(X, Y, sigmax=1.0, sigmay=1.0,\n                     mux=0.0, muy=0.0, sigmaxy=0.0):\n    \"\"\"\n    Bivariate Gaussian distribution for equal shape *X*, *Y*.\n    See `bivariate normal\n    <http://mathworld.wolfram.com/BivariateNormalDistribution.html>`_\n    at mathworld.\n    \"\"\"\n    Xmu = X-mux\n    Ymu = Y-muy\n\n    rho = sigmaxy/(sigmax*sigmay)\n    z = Xmu**2/sigmax**2 + Ymu**2/sigmay**2 - 2*rho*Xmu*Ymu/(sigmax*sigmay)\n    denom = 2*np.pi*sigmax*sigmay*np.sqrt(1-rho**2)\n    return np.exp(-z/(2*(1-rho**2))) / denom\n\ndef BivariateNormalPDFPlot():\n  # Number of points in each direction\n      n = 40;\n      \n      # parameters\n      mu_1 = 0;\n      mu_2 = 0;\n      sigma_1=1;\n      sigma_2=0.5;\n      rho1=0.0\n      rho2=-0.8\n      rho3=0.8\n      \n      x = np.linspace(-3.0,3.0,n)\n      y = np.linspace(-3.0,3.0,n)\n      X,Y =np.meshgrid(x,y)\n      Z = lambda rho:bivariate_normal(X,Y,sigma_1,sigma_2,mu_1,mu_2,rho*sigma_1*sigma_2)\n      \n      fig =plt.figure(1)\n      ax = fig.add_subplot(projection= '3d')\n      ax.plot_surface(X, Y, Z(rho1),cmap='viridis',linewidth=0)\n      ax.set_xlabel('X axis')\n      ax.set_ylabel('Y axis')\n      ax.set_zlabel('Z axis')\n      plt.show()\n      \n      fig =plt.figure(2)\n      ax = fig.add_subplot(projection= '3d')\n      ax.plot_surface(X, Y, Z(rho2),cmap='viridis',linewidth=0)\n      ax.set_xlabel('X axis')\n      ax.set_ylabel('Y axis')\n      ax.set_zlabel('Z axis')\n      plt.show()\n      \n      fig =plt.figure(3)\n      ax = fig.add_subplot(projection= '3d')\n      ax.plot_surface(X, Y, Z(rho3),cmap='viridis',linewidth=0)\n      ax.set_xlabel('X axis')\n      ax.set_ylabel('Y axis')\n      ax.set_zlabel('Z axis')\n      plt.show()\n  \nBivariateNormalPDFPlot()\n```\n\n\n\n## Hypothesis Testing\n\n<u> t-statistic </u> is the ratio of departure of the estimated value of a paramater from its hypothesized value to it's standard error.\n\nIt is used when the sample size is small or the population standard deviation is unknown. \n\nLet $\\hat\\beta$ be an estimator of parameter $\\beta$ in some statistical model. Then the t-statistic is given by $$ t_{\\hat\\beta}  = \\frac{\\hat\\beta - \\beta_0}{s.e(\\hat\\beta)}$$ where $s.e(\\hat\\beta)$ is the standard error of the estimator $\\hat\\beta$ for  $\\beta$ and $\\beta_0$ is a non-random , know constant , which may or maynot match actual unknow parameter value $\\beta$\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"toc-depth":4,"embed-resources":true,"output-file":"statistics.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.269","jupyter":"python3","theme":"sketchy","toc-location":"right","title":"Statistics"},"extensions":{"book":{"multiFile":true}}}}}