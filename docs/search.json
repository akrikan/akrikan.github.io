[
  {
    "objectID": "docu/anotherpage.html",
    "href": "docu/anotherpage.html",
    "title": "MyBlog",
    "section": "",
    "text": "Quantitative finance resources\n\n\n\n\n\n\n\nOpinion\n\n\n\n\n\n\n\n\n\n\n\nJan 2, 2023\n\n\nKrishnakant A\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "docu/about.html",
    "href": "docu/about.html",
    "title": "Krishnakant Ammanamanchi",
    "section": "",
    "text": "Hi , I am Krishnakant Ammanamanchi , welcome to my Site/Blog or whatever.\nI am unsure what to call this anymore. The gist of the site is to capture the projects and skills that i am currently working on and document them as I keep on learning as well as to write up on topics that i find fascinating.\nThere are lot sections that i have added to the site to keep myself accountable and motivated to keep posting on this site, the current goal is to populate all the sections decently enough by 2023 mid.\nFeel free to jump to any of the above sections.(Most of the sections are work in progress but eventually everything will be in order)\nHappy exploring !!!\nThis is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "docu/SEA/math.html",
    "href": "docu/SEA/math.html",
    "title": "Math",
    "section": "",
    "text": "\\alpha + \\beta\n\\Pi + \\Gamma\na \\bmod b\nx \\equiv a \\pmod{b}\n\\lim\\limits_{x \\to \\infty} \\exp(-x) =0\nk_{n+1} = n^2 +k_n^2 - k_{n-1}\n\\dots\nF_{\\ell} = \\mathbb{P}\\{X - {\\ell} \\leq{x} | X > {\\ell} \\} = \\frac{F(x+ {\\ell}) - F(\\ell)}{1 - F_{X}\\{\\ell\\}}\n\nlibrary(Sim.DiffProc)\nmu=1;sigma=0.5;theta=2\n x0=0;y0=0;init=c(x0,y0)\nf <- expression(1/mu*(theta-x), x)  \ng <- expression(sqrt(sigma),0)\nmod2d <- snssde2d(drift=f,diffusion=g,M=500,Dt=0.015,x0=c(x=0,y=0))\n## true values of first and second moment at time 10\nEx <- function(t) theta+(x0-theta)*exp(-t/mu)\nVx <- function(t) 0.5*sigma*mu *(1-exp(-2*(t/mu)))\nEy <- function(t) y0+theta*t+(x0-theta)*mu*(1-exp(-t/mu))\nVy <- function(t) sigma*mu^3*((t/mu)-2*(1-exp(-t/mu))+0.5*(1-exp(-2*(t/mu))))\ncovxy <- function(t) 0.5*sigma*mu^2 *(1-2*exp(-t/mu)+exp(-2*(t/mu)))\ntvalue = list(m1=Ex(15),m2=Ey(15),S1=Vx(15),S2=Vy(15),C12=covxy(15))\n ## function of the statistic(s) of interest.\nsde.fun2d <- function(data, i){\n   d <- data[i,]\n   return(c(mean(d$x),mean(d$y),var(d$x),var(d$y),cov(d$x,d$y)))\n}\n ## Parallel Monte-Carlo of 'OUI' at time 10\nmcm.mod2d = MCM.sde(mod2d,statistic=sde.fun2d,time=15,R=10,exact=tvalue,parallel=\"snow\",ncpus=2)\nmcm.mod2d$MC\n\n         Exact   Estimate       Bias Std.Error      RMSE\nm1   1.9999994  2.0140861  0.0140867 0.0055712 0.0218581\nm2  28.0000006 27.9721715 -0.0278291 0.0312552 0.0978083\nS1   0.2500000  0.2445866 -0.0054134 0.0038780 0.0128319\nS2   6.7500003  6.5992153 -0.1507850 0.0799199 0.2832329\nC12  0.2499998  0.2157827 -0.0342171 0.0160072 0.0589651\n           CI( 2.5 % , 97.5 % )\nm1    ( 2.0031667 , 2.0250055 )\nm2  ( 27.9109124 , 28.0334306 )\nS1    ( 0.2369859 , 0.2521873 )\nS2    ( 6.4425752 , 6.7558554 )\nC12   ( 0.1844092 , 0.2471562 )\n\n\n\\begin{equation*}\n\\begin{cases}\n\\begin{split}\ndX_{t} &= \\left( \\alpha \\, X_{t} \\, \\left( 1 - \\frac{X_{t}}{\\beta} \\right) - \\frac{\\delta \\, X_{t}^2 \\, Y_{t}}{\\left( \\kappa + X_{t}^2 \\right)} \\right) \\:dt +  \\sqrt{\\sigma_{1}} \\, X_{t} \\, \\left( 1 - Y_{t} \\right) \\:dW_{1,t} \\\\\ndY_{t} &= \\left( \\frac{\\gamma \\, X_{t}^2 \\, Y_{t}}{\\left( \\kappa + X_{t}^2 \\right)} - \\mu \\, Y_{t}^2 \\right) \\:dt +  \\left| \\sigma_{2}\\right|  \\, Y_{t} \\, \\left( 1 - X_{t} \\right) \\:dW_{2,t}\n\\end{split}\n\\end{cases}\n\\end{equation*}\n\\begin{equation}\n\\begin{cases}\n\\begin{split}\n\\frac{d}{dt} m_{1}(t) ~&= \\frac{\\left( \\theta - m_{1}(t) \\right)}{\\mu} \\\\\n\\frac{d}{dt} m_{2}(t) ~&= m_{1}(t) \\\\\n\\frac{d}{dt} S_{1}(t) ~&= \\sigma - 2 \\, \\left( \\frac{S_{1}(t)}{\\mu} \\right) \\\\\n\\frac{d}{dt} S_{2}(t) ~&= 2 \\, C_{12}(t) \\\\\n\\frac{d}{dt} C_{12}(t) &= S_{1}(t) - \\frac{C_{12}(t)}{\\mu}\n\\end{split}\n\\end{cases}\n\\end{equation}\n\\begin{equation}\n\\left.\\begin{aligned}\n        B'&=-\\partial \\times E,\\\\\n        E'&=\\partial \\times B - 4\\pi j,\n       \\end{aligned}\n\\right\\}\n\\qquad \\text{Maxwell's equations}\n\\end{equation}"
  },
  {
    "objectID": "docu/SEA/index.html",
    "href": "docu/SEA/index.html",
    "title": "Sea of Everything",
    "section": "",
    "text": "Inspired by the Sea of foundations By Professor Ravi Vakil , which covers pretty much everything one needs to master Algebraic geometry. I have titled this section as Sea of Everything where I would be covering all my current interests related to fields of financial mathematics,programming , machine learning , crypto , etc…\nMy Home page would host articles that I have written to explain myself the inner - working of an idea or a concept and which are fascinating enough to be highlighted amidst all of this.\nThe current direction of this section mostly looks like combination of notes and insights.\nI have included my hand written notes for few sections. I would be adding hand written notes and expand them as I keep on coming across various subtopics within the subject.\nInitially I had the plan to type the content but given my current over demanding job, I have decided to keep a mix of both written and typed sections.\nApologies for my handwriting it’s terrible I know.\nP.S - Looks like my aim was quite ambitious, looking at the timeframe I set for myself. My initial plan was to add the content on this website in a very organised manner starting with preliminaries and then going for the meat of the subject but lately I have realised that it has been time consuming. Hence I have decided rather to include mostly definitions and important information regarding those topics in SEA. Mostly would merge Implementations section and SEA sometime in the future but as of now they are seperate. The current outline of this website represents my vision for the site, but as I add content I would keep on shifting the structure slightly as I see fit."
  },
  {
    "objectID": "math2.html",
    "href": "math2.html",
    "title": "Krishnakant A",
    "section": "",
    "text": "\\(\\alpha + \\beta\\)\n\\(\\Pi + \\Gamma\\)\n\\(a \\bmod b\\)\n\\(x \\equiv a \\pmod{b}\\)\n\\(\\lim\\limits_{x \\to \\infty} \\exp(-x) =0\\)\n\\(k_{n+1} = n^2 +k_n^2 - k_{n-1}\\)\n\\(\\dots\\)\n\nlibrary(Sim.DiffProc)\n\nPackage 'Sim.DiffProc', version 4.8\nbrowseVignettes('Sim.DiffProc') for more informations.\n\nmu=1;sigma=0.5;theta=2\n x0=0;y0=0;init=c(x0,y0)\nf <- expression(1/mu*(theta-x), x)  \ng <- expression(sqrt(sigma),0)\nmod2d <- snssde2d(drift=f,diffusion=g,M=500,Dt=0.015,x0=c(x=0,y=0))\n## true values of first and second moment at time 10\nEx <- function(t) theta+(x0-theta)*exp(-t/mu)\nVx <- function(t) 0.5*sigma*mu *(1-exp(-2*(t/mu)))\nEy <- function(t) y0+theta*t+(x0-theta)*mu*(1-exp(-t/mu))\nVy <- function(t) sigma*mu^3*((t/mu)-2*(1-exp(-t/mu))+0.5*(1-exp(-2*(t/mu))))\ncovxy <- function(t) 0.5*sigma*mu^2 *(1-2*exp(-t/mu)+exp(-2*(t/mu)))\ntvalue = list(m1=Ex(15),m2=Ey(15),S1=Vx(15),S2=Vy(15),C12=covxy(15))\n ## function of the statistic(s) of interest.\nsde.fun2d <- function(data, i){\n   d <- data[i,]\n   return(c(mean(d$x),mean(d$y),var(d$x),var(d$y),cov(d$x,d$y)))\n}\n ## Parallel Monte-Carlo of 'OUI' at time 10\nmcm.mod2d = MCM.sde(mod2d,statistic=sde.fun2d,time=15,R=10,exact=tvalue,parallel=\"snow\",ncpus=2)\nmcm.mod2d$MC\n\n         Exact   Estimate       Bias Std.Error      RMSE\nm1   1.9999994  2.0012785  0.0012791 0.0071115 0.0213728\nm2  28.0000006 27.9709325 -0.0290681 0.0461900 0.1415859\nS1   0.2500000  0.2446978 -0.0053022 0.0061740 0.0192661\nS2   6.7500003  6.3938382 -0.3561621 0.1191446 0.5045894\nC12  0.2499998  0.2117307 -0.0382691 0.0130157 0.0546737\n           CI( 2.5 % , 97.5 % )\nm1    ( 1.9873402 , 2.0152168 )\nm2  ( 27.8804018 , 28.0614632 )\nS1     ( 0.232597 , 0.2567986 )\nS2    ( 6.1603191 , 6.6273573 )\nC12    ( 0.1862204 , 0.237241 )\n\n\n\\[\\begin{equation*}\n\\begin{cases}\n\\begin{split}\ndX_{t} &= \\left( \\alpha \\, X_{t} \\, \\left( 1 - \\frac{X_{t}}{\\beta} \\right) - \\frac{\\delta \\, X_{t}^2 \\, Y_{t}}{\\left( \\kappa + X_{t}^2 \\right)} \\right) \\:dt +  \\sqrt{\\sigma_{1}} \\, X_{t} \\, \\left( 1 - Y_{t} \\right) \\:dW_{1,t} \\\\\ndY_{t} &= \\left( \\frac{\\gamma \\, X_{t}^2 \\, Y_{t}}{\\left( \\kappa + X_{t}^2 \\right)} - \\mu \\, Y_{t}^2 \\right) \\:dt +  \\left| \\sigma_{2}\\right|  \\, Y_{t} \\, \\left( 1 - X_{t} \\right) \\:dW_{2,t}\n\\end{split}\n\\end{cases}\n\\end{equation*}\\]\n\\[\\begin{equation}\n\\begin{cases}\n\\begin{split}\n\\frac{d}{dt} m_{1}(t) ~&= \\frac{\\left( \\theta - m_{1}(t) \\right)}{\\mu} \\\\\n\\frac{d}{dt} m_{2}(t) ~&= m_{1}(t) \\\\\n\\frac{d}{dt} S_{1}(t) ~&= \\sigma - 2 \\, \\left( \\frac{S_{1}(t)}{\\mu} \\right) \\\\\n\\frac{d}{dt} S_{2}(t) ~&= 2 \\, C_{12}(t) \\\\\n\\frac{d}{dt} C_{12}(t) &= S_{1}(t) - \\frac{C_{12}(t)}{\\mu}\n\\end{split}\n\\end{cases}\n\\end{equation}\\]\n\\[\\begin{equation}\n\\left.\\begin{aligned}\n        B'&=-\\partial \\times E,\\\\\n        E'&=\\partial \\times B - 4\\pi j,\n       \\end{aligned}\n\\right\\}\n\\qquad \\text{Maxwell's equations}\n\\end{equation}\\]\n\n# Load required packages\nlibrary(fpp3)\n\n── Attaching packages ──────────────────────────────────────────── fpp3 0.4.0 ──\n\n\n✔ tibble      3.1.8      ✔ tsibble     1.1.3 \n✔ dplyr       1.0.10     ✔ tsibbledata 0.4.1 \n✔ tidyr       1.2.1      ✔ feasts      0.3.0 \n✔ lubridate   1.9.0      ✔ fable       0.3.2 \n✔ ggplot2     3.4.0      \n\n\n── Conflicts ───────────────────────────────────────────────── fpp3_conflicts ──\n✖ lubridate::date()    masks base::date()\n✖ dplyr::filter()      masks stats::filter()\n✖ tsibble::intersect() masks base::intersect()\n✖ tsibble::interval()  masks lubridate::interval()\n✖ dplyr::lag()         masks stats::lag()\n✖ tsibble::setdiff()   masks base::setdiff()\n✖ tsibble::union()     masks base::union()\n\n# Plot one time series\naus_retail %>%\n  filter(`Series ID`==\"A3349640L\") %>%\n  autoplot(Turnover)\n\n\n\n# Produce some forecasts\naus_retail %>%\n  filter(`Series ID`==\"A3349640L\") %>%\n  model(ETS(Turnover)) %>%\n  forecast(h = \"2 years\")\n\n# A fable: 24 x 6 [1M]\n# Key:     State, Industry, .model [1]\n   State    Industry                          .model    Month     Turnover .mean\n   <chr>    <chr>                             <chr>     <mth>       <dist> <dbl>\n 1 Victoria Cafes, restaurants and catering … ETS(T… 2019 Jan  N(608, 978)  608.\n 2 Victoria Cafes, restaurants and catering … ETS(T… 2019 Feb N(551, 1129)  551.\n 3 Victoria Cafes, restaurants and catering … ETS(T… 2019 Mar N(622, 1856)  622.\n 4 Victoria Cafes, restaurants and catering … ETS(T… 2019 Apr N(609, 2190)  609.\n 5 Victoria Cafes, restaurants and catering … ETS(T… 2019 May N(602, 2539)  602.\n 6 Victoria Cafes, restaurants and catering … ETS(T… 2019 Jun N(577, 2704)  577.\n 7 Victoria Cafes, restaurants and catering … ETS(T… 2019 Jul N(607, 3413)  607.\n 8 Victoria Cafes, restaurants and catering … ETS(T… 2019 Aug N(626, 4072)  626.\n 9 Victoria Cafes, restaurants and catering … ETS(T… 2019 Sep N(614, 4358)  614.\n10 Victoria Cafes, restaurants and catering … ETS(T… 2019 Oct N(624, 4942)  624.\n# … with 14 more rows"
  },
  {
    "objectID": "docu/Book_Implementations/index.html",
    "href": "docu/Book_Implementations/index.html",
    "title": "Book Implementations",
    "section": "",
    "text": "As the name suggests this section deals with various technical books that have read and most importantly their code implementation.\nCurrent plan is write code and look at the code."
  },
  {
    "objectID": "docu/SEA/Q_F/index.html#title-quantitative-finance",
    "href": "docu/SEA/Q_F/index.html#title-quantitative-finance",
    "title": "Krishnakant A",
    "section": "title: “Quantitative Finance”",
    "text": "title: “Quantitative Finance”\nThis topic mainly covers stochastic calculus , I find the theory and its applications within finance quite interesting, the reason why I have to not included this section inside mathematics is mostly because I stumbled across this topic when I had started reading option theory, though i had heard of it during my graduate program but never found it interesting and never gave a second thought about, Interestingly enough I got interested in the topic once I realised I won’t be pursuing a PH.D in Math.\nThere is(was) a great course on Stochastic process by Higher School of Economics(Russia) on coursera which i happened to complete.\nI have as of now also included Econometrics as part of Quant finance against popular opinion since everything is just a linear regression.\nFuture topics might include Market Risk Analysis , the four volume set on Market risk analysis by Carol Alexander are a gem. I happened to come across it while i was preparing for my FRM exams."
  },
  {
    "objectID": "docu/SEA/Q_F/index.html",
    "href": "docu/SEA/Q_F/index.html",
    "title": "Quantitative Finance",
    "section": "",
    "text": "This topic mainly covers stochastic calculus , I find the theory and its applications within finance quite interesting, the reason why I have decided to not included this section inside mathematics is mostly because I stumbled across this topic when I had started reading option theory, though i had heard of it during my graduate program but never found it interesting and never gave a second thought about it. Oddly enough I got fascinated with the topic once I realised I won’t be pursuing a PH.D in Math.\nThere is(was) a great course on Stochastic process by Higher School of Economics(Russia) on coursera which i happened to complete.\nI have as of now also included Econometrics as part of Quant finance against popular opinion since everything is just a linear regression.\nFuture topics might include Market Risk Analysis , the four volume set on Market risk analysis by Carol Alexander is a gem. I happened to come across it while i was preparing for my FRM exams."
  },
  {
    "objectID": "docu/posts/firstpost.html",
    "href": "docu/posts/firstpost.html",
    "title": "Quantitative finance resources",
    "section": "",
    "text": "Quantitative finance is vast field with lot many resources available. Many would agree these resources would help one get a job and be at intermediate level in terms of expertise, while becoming a expert is still sort of restricted to learning on the job. But can one ever become an expert in any chosen topic within Quantitative finance or even for that matter get to an intermediate level of understanding of various topics within the field.\n\n\n\n\nOne of the most difficult decisions that I had to make while venturing into this field was choose a book to read or videos to watch and stick to it without regretting later that it wasn’t enough or worse I chose the wrong resource. Educating oneself for the purpose of research in purely theoretical field is far more simple, you know for sure the list of prerequistes for the topic , the books that you need to master the topic, current and past important research papers you need to read and research work by list of researchers you need to follow voilà you are close to becoming an expert in the field given of course you have had comprehensive understanding of the topic which you are able explain with relative ease. A researcher lets say in the field of pure mathematics doesnt care whether his research is going to be relevant in next 10 years or 20 years,for him the joy lies in solving the problem and coming up with innovative solution , it hardly matters whether his research topic will be relevant for he has gained a level of expertise to such an extent that such a thing were ever to bother him he would just shift his focus in a closely related field(assuming there is always a closely related field) of relevance with minimal difficulty and at time same time continue doing research on the topic and hoping there would be someone who might pursue it further later on.\nThis approach of the researcher I think is the best way to move forward in learning any topic. Make yourself an expert in the basics of the field and you are free to move around without the overhead of being relevant or for that matter employable. To worry about whether the resource you have chosen to master a topic is enough or not gets you no where. \nThere are lot many topics within the field of quantitative finance for one to come closer to becoming an expert.\nOn the internet you will find lot many articles listing resources based on each topic. In the end they even go on to say the list is not exhaustive making even more difficult to choose. I think its nearly impossible to cover that many books( they could just be resources for reference but that’s hardly ever the tone of those articles) in any reasonable amount of time.\nYou can refer to the resources section up top for list of all books , articles , youtube video that I have come across over the period of time. Here I have just listed down couple of books on each topic and contents of each topics that one needs to master(based on job description for almost any type of quantitative role) and that can be completed in a reasonable amount of time(give yourself a year or six months if you are really comfortable with the math ). I have included books that are at times quite math heavy since I prefer them more and I have a background(Masters) in pure(non-applicable) mathematics. I would suggest to get comfortable with math for it becomes easier in manipulating models and creating new ones from the existing models without much effort. This is particularly true in this field as one proceeds with the assumption that all models are wrong, math would help you in converging to the reality (ironically).Try to implement the mathematical topics in a programming language of your choice instead of postponing it.\nHere is the list of resources that I am currently reading and following:\n\nStatistics and probability\nA. Topics\n\nProbability and distributions (All the common and standard distributions)\nMultivariate distributions\nStastical inference\nMaximum likelikhood methods\nSufficiency\nGeneralised linear models\nLimiting Distributions\nOptimal tests of hypothesis\nNon parametric and robust statistics\n\nB. Books and videos\n\nIntroduction to mathematical statistics by Hogg ,Mckean and Craig\nAll of Statistics by L. Wasserman\nMathematical theory of Bayesian Mathematics by Sumio Watanabe\nStatistics for Application by Philippe Rigollet (Youtube video - MIT OCW)\n\nMachine learning and deep learning\nA. Topics\n\nSupervised learning algorithms\nUnsupervised Learning Algorithms\nAdvanced learning algorithims\n\nB Books and Videos\n\n[Theory]Probabilistic Machine learning Book 1 by Kevin Murphy\n[Theory]Understanding Machine learning theory algorithims\nISLR[Introduction to statistical learnign - good for R implementation] and ESL[Elements of statistical learning- Bible of Statistical Learning] by Hastie and Tibshirani\n[Implementation] Hands-On Machine Learning with Scikit-Learn, Kera & Tensorflow by Aurelien Geron\n[Bonus Implementation] Machine Learing in C++ by Kiril Kolodiazhnyi\n[Coursera - Deep Learning.ai - Andrew Ng] Machine Learning Specialization(This course enough for all intents and purposes unless you wish to delve deeper)\n\nStochastic calculus[An absolute on Sell Side]\nA. Topics\n\nMeasure theoretic Probability\nBrownian Motion\nIto’s Integral [the most important subtopic]\nIto-Doeblin Formula\nLevy Process and Jump Process\nChange of Numeraire\nTerm Structure Models\n\nB. Books and videos\n\nStochastic Calculus for finance II by Steven E Shreve [This is enough for all intents and purposes] but if you are interested in Levy processes than you might have to refer to another book.\nMathematical Modeling and computation in finance by Cornelis W Oosterlee and Lech A Grezelak (You can find videos which follow the book by the later author on you tube making it one of the most approachable resource on the topic) I would sugget to go through this text once, before you go for Shreve’s book. Shreve’s book extensively makes use of Measure Theory.Great theoretical and practical book. I personally use this book and shreve’s book for refernce. TO be honest no stochastic calculus for finance book actually is self contained, many theorems are referenced.\nPricing options with Mathematical Models (great course on coursera ) from Caltech.\n\nFinancial products (Mostly Options)\nProgramming tools\nA. Topics\nScientific Computing\nTradFi(Traditional Finance)"
  },
  {
    "objectID": "docu/Twitter_SS.html",
    "href": "docu/Twitter_SS.html",
    "title": "Resources(Mostly Twitter S/S)",
    "section": "",
    "text": "This page basically contains twitter threads screen shots , there is no easy way to save twitter threads on twitter and save them or at least i haven’t bothered to search for a solution ,so anyways would be including it here\nAs of now I haven’t categorized it and maybe in future will add links and reviews to various resources which i have found useful over time.\nThe date, time and twitter_handle is included in the screen-shot so you can go ahead and search for it , I have not purposefully included them the links since many a times the author deletes the tweets and it becomes a headache to revist dead-links and remove them , trying to keep it simple here."
  },
  {
    "objectID": "docu/Resources/index.html",
    "href": "docu/Resources/index.html",
    "title": "Resources(Mostly Twitter S/S)",
    "section": "",
    "text": "This page basically contains every form of resource that I have come across while learning and studying various topics in all fields that interest me , not specifically restricted to quantitative finance."
  },
  {
    "objectID": "docu/SEA/time_series.html",
    "href": "docu/SEA/time_series.html",
    "title": "Time Series Analysis",
    "section": "",
    "text": "Time series analysis by James D Hamilton - its considered to be the bible on time series analysis, pretty much covers all the theory , it doesnt have any code or pseudocode\nIntroduction to Time Series with R by Metcalfe & Coweperwait\nAnalysis of Financial Time Series by Tsay\n\n\n\n\n\nWould be exploring time series analysis in the context of finance\nImplementing code from the book by Tsay , would be using the book by Hamilton and Metcalfe as reference\nAim to understand different time series models in R.\n(Will keep on adding the aims as is go on)"
  },
  {
    "objectID": "docu/SEA/MRA.html",
    "href": "docu/SEA/MRA.html",
    "title": "Market Risk Analysis",
    "section": "",
    "text": "The best book i found for this topic is by Prof. Carol Alexanders , its a four volume text. Recently she made the accompanying excel files available for free making the text and content even more approachable.\nWould be adding topics of interest from those books in various sections of this site"
  },
  {
    "objectID": "docu/SEA/time_series.html#part-1",
    "href": "docu/SEA/time_series.html#part-1",
    "title": "Time Series Analysis",
    "section": "Part 1",
    "text": "Part 1\n\nDifference operators\nasd\n\n\nLag operators\nsdf\n\n\nStationary ARMA Process\nsadf\n\n\nForecasting\nsadf\n\n\nMaximum likelihood Estimation\nsadf\n\n\nSpectral Analysis\nasdf"
  },
  {
    "objectID": "docu/Resources/index.html#quantitative-finance",
    "href": "docu/Resources/index.html#quantitative-finance",
    "title": "Resources(Mostly Twitter S/S)",
    "section": "Quantitative Finance",
    "text": "Quantitative Finance\n\nStochastic Calculus"
  },
  {
    "objectID": "docu/Resources/index.html#mathematics",
    "href": "docu/Resources/index.html#mathematics",
    "title": "Resources",
    "section": "Mathematics",
    "text": "Mathematics\n\nStatistics"
  },
  {
    "objectID": "docu/Resources/index.html#programming",
    "href": "docu/Resources/index.html#programming",
    "title": "Resources",
    "section": "Programming",
    "text": "Programming\n\nR\n\n\nPython\n\n\nC+++\n\n\nRust"
  },
  {
    "objectID": "docu/Resources/index.html#data-science-machine-learning-and-deep-learning",
    "href": "docu/Resources/index.html#data-science-machine-learning-and-deep-learning",
    "title": "Resources",
    "section": "Data Science , Machine Learning and Deep learning",
    "text": "Data Science , Machine Learning and Deep learning"
  },
  {
    "objectID": "docu/Resources/index.html#trading",
    "href": "docu/Resources/index.html#trading",
    "title": "Resources",
    "section": "Trading",
    "text": "Trading"
  },
  {
    "objectID": "docu/Resources/index.html#tradfi-and-finance-books",
    "href": "docu/Resources/index.html#tradfi-and-finance-books",
    "title": "Resources",
    "section": "TradFi and Finance books",
    "text": "TradFi and Finance books"
  },
  {
    "objectID": "docu/SEA/python.html",
    "href": "docu/SEA/python.html",
    "title": "Programming language",
    "section": "",
    "text": "High Level language - Strong abstraction, easy to use and easy to understand syntax.\nGeneral purpose programming language\nIndentation important\nCase Sensitive\nSupports multiple programming paradigms such OOP and functional\nExcellently documented and comprehensive standard library\nDynamically but Strongly typed - Types are checked at run time and forbids operations that are not of the same type\nGarbage collected - Automatic memory management\n\n\n\n\n\nThe Python Standard Library\n\nBuilt-in types\n\nlist ,tuple, set, dict and others\n\nBuilt-in functions\n\nprint() , len() ,range(),enumerate(),map(),zip() and others\n\nBuilt-in modules\n\nos,sys,itertools,collecations,math and others"
  },
  {
    "objectID": "docu/SEA/StochC.html",
    "href": "docu/SEA/StochC.html",
    "title": "Stochastic Calculus",
    "section": "",
    "text": "These are list of books , videos and courses that I am currently referencing for content on this page , for more detailed list please check out the resource section above.\n\nMathematical modelling and computation in finance by Cornerlis Ooseterlee and Lech Grzelak (Main Text)\nStochastic CalcuIus for Finance II Continuous-Time Models by Steven Shreve (Main Reference text)\nStochastic Differential Equations By Bernt Oksendal"
  },
  {
    "objectID": "docu/SEA/StochC.html#algebra",
    "href": "docu/SEA/StochC.html#algebra",
    "title": "Stochastic Calculus",
    "section": "algebra",
    "text": "algebra\nLet be a nonempty set , and let"
  },
  {
    "objectID": "docu/SEA/StochC.html#line",
    "href": "docu/SEA/StochC.html#line",
    "title": "Stochastic Calculus",
    "section": "Line",
    "text": "Line\nThe equation of any straight line, called a linear equation, can be written as:\n\ny = mx + b\n :::\nSee ?@def-line."
  },
  {
    "objectID": "docu/SEA/StochC.html#sigma-algebra",
    "href": "docu/SEA/StochC.html#sigma-algebra",
    "title": "Stochastic Calculus",
    "section": "\\sigma algebra",
    "text": "\\sigma algebra\nLet \\Omega be a nonempty set , and let \\mathbb{F} \ny = mx + b"
  },
  {
    "objectID": "docu/SEA/Machine Learning.html",
    "href": "docu/SEA/Machine Learning.html",
    "title": "Machine Learning",
    "section": "",
    "text": "##Introduction\n##Resources\n##Supervised Learning \n###Linear Regression\n###Logistic Regression"
  },
  {
    "objectID": "docu/SEA/Machine Learning.html#resources",
    "href": "docu/SEA/Machine Learning.html#resources",
    "title": "Machine Learning",
    "section": "Resources",
    "text": "Resources\n\nBooks:\n\nIntroduction to Statistical Learning (ISLR) Hastie and Tibshirani(R implementation and relatively less dense than ESL )\nElements of Statistical Learning By Hastie and Tibshirani (The Mathematical theory)"
  },
  {
    "objectID": "docu/SEA/Machine Learning.html#supervised-learning",
    "href": "docu/SEA/Machine Learning.html#supervised-learning",
    "title": "Machine Learning",
    "section": "Supervised Learning",
    "text": "Supervised Learning\n\n\nLinear Regression\n\n\nLogistic Regression"
  },
  {
    "objectID": "docu/SEA/python.html#basics",
    "href": "docu/SEA/python.html#basics",
    "title": "Python Programming language",
    "section": "Basics",
    "text": "Basics\n\nThe Python Standard Library\n\nBuilt-in types\n\nlist ,tuple, set, dict and others\n\nBuilt-in functions\n\nprint() , len() ,range(),enumerate(),map(),zip() and others\n\nBuilt-in modules\n\nos,sys,itertools,collecations,math and others"
  },
  {
    "objectID": "docu/SEA/practice.html",
    "href": "docu/SEA/practice.html",
    "title": "Quarto HTML Basics",
    "section": "",
    "text": "This a Quarto document. To learn more about Quarto see https://quarto.org.\nClick the Code button in the header to see the full source code of this document.\nHere we call the R summary() function—the function’s output is included immediately below:\n\nsummary(cars)"
  },
  {
    "objectID": "docu/SEA/practice.html#plot-output",
    "href": "docu/SEA/practice.html#plot-output",
    "title": "Quarto HTML Basics",
    "section": "Plot Output",
    "text": "Plot Output\nYou can also embed plots, for example:\n\nlibrary(ggplot2)\ndat <- data.frame(cond = rep(c(\"A\", \"B\"), each=10),\n                  xvar = 1:20 + rnorm(20,sd=3),\n                  yvar = 1:20 + rnorm(20,sd=3))\n\nggplot(dat, aes(x=xvar, y=yvar)) +\n    geom_point(shape=1) + \n    geom_smooth() \n\nNote that the code-fold: true parameter was added to the code chunk to hide the code by default (click “Code” above the plot to see the code).\nThe use of the label and fig-cap options make this a cross-referenceable figure (see )."
  },
  {
    "objectID": "docu/SEA/practice.html#interactivity",
    "href": "docu/SEA/practice.html#interactivity",
    "title": "Quarto HTML Basics",
    "section": "Interactivity",
    "text": "Interactivity\nYou can also add interactive plots. For example:\n\nlibrary(dygraphs)\ndygraph(nhtemp) %>% \n  dyRangeSelector(dateWindow = c(\"1920-01-01\", \"1960-01-01\"))"
  },
  {
    "objectID": "docu/SEA/practice.html#tables",
    "href": "docu/SEA/practice.html#tables",
    "title": "Quarto HTML Basics",
    "section": "Tables",
    "text": "Tables\nUse the knitr::kable() function to print tables as HTML:\n\nknitr::kable(head(ggplot2::diamonds))"
  },
  {
    "objectID": "docu/SEA/practice.html#latex-math",
    "href": "docu/SEA/practice.html#latex-math",
    "title": "Quarto HTML Basics",
    "section": "LaTeX Math",
    "text": "LaTeX Math\nYou can also include LaTeX math:\n\nP\\left(A=2\\middle|\\frac{A^2}{B}>4\\right)"
  },
  {
    "objectID": "docu/SEA/StochC.html#general-probability-theory",
    "href": "docu/SEA/StochC.html#general-probability-theory",
    "title": "Stochastic Calculus",
    "section": "General Probability theory",
    "text": "General Probability theory\n\nDefinition 1 (\\sigma algebra) : Let \\Omega be a nonempty set , and let \\mathcal{F} be a set of collections of subsets of \\Omega. We say that \\mathcal{F} is a \\sigma algebra provided that\n\nThe empty set \\emptyset belongs to \\mathcal{F}\nA and A^{c} belong to \\mathcal{F} whenever A \\in \\mathcal{F}\nwhenever A_1 ,A_2 ,A_3 ..... \\in \\mathcal{F} then \\bigcup_{i=1}^\\infty A_{i} \\in \\mathcal{F}\n\n\n \n\nDefinition 2 (Probability measure) : Let \\Omega be a nonempty set , and let \\mathbb{F} be \\sigma algebra of subsets of \\Omega. A probability measure \\mathbb{P} is a function that, to every set A \\in \\mathcal{F} , assigns a number in [0,1] called the probability of A written as \\mathbb{P}(A) .We require\n\n\\mathbb{P}(\\Omega) = 1 and\n(Countable Additivity) Whenever A_1,A_2,A_3.... is a sequence of disjoint sets in \\mathcal{F} , then \\mathbb{P}(\\bigcup_{n=1}^\\infty A_n) = \\sum_{n=1}^{\\infty} \\mathbb{P}(A_n) \n\n The Triple (\\Omega, \\mathcal{F},\\mathbb{}P) is called probability measure\n\n From the above it follows that for finitely many disjoint sets A_1, A_2,A_3....A_n \\mathbb{P}(\\bigcup_{n=1}^N) = \\sum_{n=1}^N \\mathbb{P}(A_n)\nand \\mathbb{P}(A^c) = 1 - \\mathbb{P}(A)\n\nExample 1 (Uniform(Lebesgue) measure on [0,1])  \\Omega = [0,1] \\mathbb{P}[a,b] = b-a ,0 \\leq a \\leq b \\leq1  single points have zero probability and  (a,b) can be written as  \\bigcup_{n=1}^\\infty[a+\\frac{1}{n} , b -\\frac{1}{n}]\n\n  The \\sigma-algebra beginning with closed intervals and adding everything else necessary in order to have a \\sigma- algebra is called a Borel \\sigma-algebra and denoted by \\mathcal{B}[0,1]. Borel \\sigma-algebras are \\sigma-algebra generated by open sets and equivalently closed sets over any topological space.\n\nDefinition 3 let (\\Omega,\\mathcal{F},\\mathbb{P}) be a probability space. If a set A \\in \\mathcal{F} satisfies \\mathbb{P}(A) = 1, we say the event A occurs almost surely\n\n\nExample 2 Let \\mathbb{P} be the uniform measure as defined in example 1. Define X(\\omega) = \\omega and Y(\\omega) = 1 - \\omega for all \\omega \\in [0,1] , then the distribution measure of X is uniform \\mu_{X}[a,b] = \\mathbb{P}\\{\\omega;a\\leq X(w)\\leq b\\} = \\mathbb{P}[a,b] = b-a, 0\\leq a \\leq b \\leq 1 by definition of \\mathbb{P}.  Its easy to see to see that X and Y have the same distribution. But under the probability measure\\mathbb{\\widetilde{P}} on [0,1] defined by \\mathbb{\\widetilde{P}[a,b]} = \\int_a^b 2\\omega \\, d\\omega = b^2-a^2  , 0 \\leq a\\leq b \\leq1  X and Y have different distributions.\n\n\nDefinition 4 (cumulative distribution function and density function)  F(x) = \\mathbb{P}\\{X \\leq x\\} , x \\in \\mathbb{R} \\mu_X[a,b] = \\mathbb{P}\\{a \\leq X \\leq b \\} = \\int_a^b f(x) \\, dx , -\\infty < a \\leq b < \\infty f(x) is called the density funtion and F(x) is called the cummulative distribution function.\n\n\nExample 3 (Standard normal random variable) Let \\phi(x)  = \\frac {a}{\\sqrt{2\\pi}}e^{\\frac{-x^2}{2}} be the standard normal density and definiing the cummulative normal distribution function as N(x) = \\int_{-\\infty}^x \\phi(\\xi)\\,d\\xi The function N(x) is strictly increasing function which is surjective onto (0,1) from \\mathbb{R}, so it has a strictly increasing inverse function N^{-1}(y),y \\in (0,1). Let Y be a uniformly distributed random variable defined on some probability space (\\Omega,\\mathcal{F},\\mathbb{P}) and set X = N^{-1}(Y) then\n\\begin{align*}\n\n\\mu_X[a,b] &= \\mathbb{P}\\{\\omega \\in \\Omega ; a \\leq X(\\omega) \\leq b\\} \\\\\n\n          &= \\mathbb{P}\\{\\omega \\in \\Omega ; a \\leq N^{-1}(Y(\\omega)) \\leq b\\}\\\\\n            \n          &= \\mathbb{P}\\{\\omega \\in \\Omega ; N(a) \\leq Y(\\omega) \\leq N(b) \\} \\\\\n        \n          &= N(b) - N(a) \\\\\n          &= \\int_a^b \\phi(x) \\, dx\n\n\\end{align*}\nAny random variable that has this distribution regardless of the probability space is called standard normal distribution. Thing to note the use of uniformly distributed random variable for generating a standard random variable is called probability integral transform and this is commonly used in Monte Carlo simulation"
  },
  {
    "objectID": "docu/SEA/index.html#learning-approach",
    "href": "docu/SEA/index.html#learning-approach",
    "title": "Sea of Everything",
    "section": "Learning approach",
    "text": "Learning approach\nI would be going with top - down approach rather than a bottoms-up approach, a quicker way surely would be sacrificing some important theorems along the way if there are not used in any models , but for all practical purposes bottoms-up approach with a full time job is time consuming and I would have to wait almost a year to even see models and implementation going by that route."
  },
  {
    "objectID": "docu/SEA/StochC.html#brownian-motion",
    "href": "docu/SEA/StochC.html#brownian-motion",
    "title": "Stochastic Calculus",
    "section": "Brownian Motion",
    "text": "Brownian Motion"
  },
  {
    "objectID": "docu/SEA/StochC.html#learning-approach",
    "href": "docu/SEA/StochC.html#learning-approach",
    "title": "Stochastic Calculus",
    "section": "Learning approach",
    "text": "Learning approach\nI would be going with top - down approach rather than a bottoms-up approach, a quicker way surely would be sacrificing some important theorems along the way if they are not used in any models , but for all practical purposes bottoms-up approach with a full time job is time consuming and I would have to wait almost a year to even see models and implementation going by that route."
  },
  {
    "objectID": "docu/SEA/financial_products.html",
    "href": "docu/SEA/financial_products.html",
    "title": "Financial Products",
    "section": "",
    "text": "Option Volatility and Pricing by Sheldon Natenberg\nOptions, Futures and Other Derivatives by John C. Hull"
  },
  {
    "objectID": "docu/SEA/financial_products.html#content",
    "href": "docu/SEA/financial_products.html#content",
    "title": "Financial Products",
    "section": "Content",
    "text": "Content"
  },
  {
    "objectID": "docu/SEA/StochC.html#black-scholes-model",
    "href": "docu/SEA/StochC.html#black-scholes-model",
    "title": "Stochastic Calculus",
    "section": "Black-Scholes model",
    "text": "Black-Scholes model"
  },
  {
    "objectID": "docu/SEA/Crypto/evmdefi.html",
    "href": "docu/SEA/Crypto/evmdefi.html",
    "title": "DEFI and its related content",
    "section": "",
    "text": "[Motivation]Order book mechaisms are dominant medium of exchange of electronic assets in traditional finance but are quite challenging to use in smart contract environmen since the size of the state needed by an order book to represent the set of outstanding orders is large and extremely costly in the smart contract environment as users must pay for space and compute power utilized.\nA constant product market maker is a market for trading coins of type \\(\\alpha\\) for coins of type \\(\\beta\\). This market has reserves \\(R_\\alpha > 0\\) $R_> 0 $, contstant product \\(k = R_\\alpha R_\\beta\\), and percentage fee \\((1-\\gamma)\\). A transaction in this market , trading $> 0 $ coins \\(\\beta\\) for $> 0 $ for coins \\(\\alpha\\) , must satisfy \\[ (R_\\alpha - \\Delta_\\alpha)(R_\\beta + \\gamma\\Delta_\\beta) = k\\] Its clear to see that the constant terms comes from the product of the two reserves at any given time is always equal to \\(k\\)"
  },
  {
    "objectID": "docu/SEA/time_series.html#financial-time-series-and-linear-time-series",
    "href": "docu/SEA/time_series.html#financial-time-series-and-linear-time-series",
    "title": "Time Series Analysis",
    "section": "Financial Time Series and Linear Time series",
    "text": "Financial Time Series and Linear Time series\n\nAsset Returns\n\nDefinition 1 (Simple gross return)  Arithmetic Returns\nP_t = P_{t-1}(1+R_t) \\implies R_t = \\frac{P_t - P_{t-1}}{P_t} where P_t and R_t are the price and return at time t respectively  Continuously compounded return or log return\nr_t = \\ln(1+R_t)  Dividend Payment\nr_t = \\ln(P_t + D_t) -\\ln(P_{t-1}) Where D_t is dividend payment between time t-1 and time t .\n\n\n\nDefinition 2 (Multiperiod gross return)  Similiary we can define multiperiod simple return by  R_t[k] = exp{[\\frac{1}{k}\\sum_{j=0}^{k-1}(1+R_{t-j})]} -1 where exp is the exponentential function and k is total number periods the asset was held for.\nLog return would be  r_t[k] = r_t +r_{t-1}+.....+r_{t-k+1}\n\n\nDefinition 3 (Portfolio Return) R_{p,t}= \\sum_{i=1}^{N}w_i R_{it}\nWhere p is the portfolio , i the ith asset and w_i the weight of the ith asset in the portofolio\n\n* Load data and header =T give the 1st row of the data file , that is the names of the cloumns of the data set\n\n# da contains the return for 5 five stocks and indexes namely IBM ,value-weighted , equal-weighted and S&P composite index from 1970 to 2008\nlibrary(fBasics)\nda = read.table(\"TST/d-ibm3dx7008.txt\",header = T) \ndim(da) # dimensions for the data\n\n[1] 9845    5\n\nda[1:5,]# first five rows\n\n\n\n  \n\n\ntail(da)#Last 5 rows\n\n\n\n  \n\n\nibm = da[,2] # IBM simple returns\n\nsibm = ibm*100 #Percentage simple returns \n\nbasicStats(sibm)\n\n\n\n  \n\n\ns1 = skewness(sibm)\nt = s1/sqrt(6/9845) #defintion for test statistic here 9845 is N \nt\n\n[1] 2.487093\nattr(,\"method\")\n[1] \"moment\"\n\npv  = 2*(1-pnorm(t)) # Calculate p-value\npv\n\n[1] 0.01287919\nattr(,\"method\")\n[1] \"moment\"\n\n\n\nlibrary(ggplot2)\nibm = as.data.frame(da[,1:2])\n\nibm[\"log_return\"] = log(1+ibm[\"rtn\"])\n\ncolnames(ibm)[2] <- \"simple_return\"\n\nggplot(ibm,aes(x = simple_return,colour =\"Emperical\"))+\nggtitle(\"Simple return distribution\") + \nxlim (-0.15,0.15)+\ngeom_density()+\nstat_function(mapping = aes(colour = \"Normal\"),\n              fun = dnorm,args = with(ibm,c(mean =mean(simple_return),\n             sd = sd(simple_return))))\nggplot(ibm,aes(x = log_return,colour = \"Emperical\"))+\nggtitle(\"Log return distribution\") + \nxlim (-0.15,0.15)+geom_density()+\nstat_function(mapping = aes(colour = \"Normal\"),\n             fun = dnorm,args = with(ibm,c(mean = mean(log_return),\n             sd = sd(log_return))))+ \nscale_x_continuous(\"log return distribution\",limits = c(-0.15,0.15))\n\n\n\n\n\n\n\n\n\n\n\n\nIBM = ibm[, 2]\n\n\nIBM = ts(IBM,frequency = 250 ,start = c(1970,1))\n\nplot.ts(IBM)\n\n\n\nacf(IBM,lag=250,ylim= c(-0.05,0.05))#ACF is covered in detail down below , the x axis label is fraction of unit time \n\n\n\n\n\n\nLinear Time Series\n Stationarity\nFoundation of time series analysis is Stationarity. A time series \\{r_t\\} is said to be strictly stationary if the joint distribution of (r_{t_{1}},r_{t_{2}},r_{t_{3}},....,r_{t_{k}}) is identical to that of (r_{t_{1+t}},r_{t_{2+t}},r_{t_{3+t}},....,r_{t_{k+t}}) for all t , where k is an arbitrary positive integer and (t_1,t_2,t_3,....,t_k) is collection of k positive integers. Strict stationarity is very hard to verify emperically  A time series is said to be weakly stationary if the mean of r_t and the covariance between r_t and r_{t-l} are time invariant where l is an arbitrary integer. Weak stationarity enables us to make inference concerning future observations\n\n\\mathbb{E}(t_t) = \\mu\nCov(r_t,t_{t-l}) = \\gamma_l\n\nWeak stationarity is commonly studied and more practical.\nThe covariance \\gamma_l = Cov(r_t,r_{t-l}) is called the lag-l autocovariance of r_t. From the above defintion it follows that\n\n\\gamma_0=Var(r_t)\n\\gamma_{-l} = \\gamma_l\n\n Autocorrelation Function\nConsider a weakly stationary return series r_t. When the linear dependence between r_t and its past values r_{t-l} is of interest , the concept of correlation is generalised to autocorrelation. The correlation coefficient between r_t and r_{t-l} is called the lag-l autocorrelation of r_t and is commonly denoted by \\rho_l , which under the weak stationarity assumption is a function of l only. Its defined by  \\rho_l = \\frac{Cov(r_t,r_{t-l})}{\\sqrt{Var(r_t)Var(r_{t-l})}} = \\frac{Cov(r_t,r_{t-l})}{Var(r_t)} = \\frac{\\gamma_l}{\\gamma_0}\nIts easy to see that\n\n\\rho_0 = 1\n\\rho_l = \\rho_{-l}\n-1\\leq \\rho_l \\leq 1\n\nFor a given sample of returns \\{r_t\\}_{t=1}^T , let \\bar r be the sample mean. Then the lag-1 autocorrelation of r_t is \\hat\\rho_1 = \\frac{\\sum_{t=2}^T(r_t - \\bar r)(r_{t-1} - \\bar r)}{\\sum_{t=2}^T(r_t - \\bar r )^2}\nThe conditions under which \\hat\\rho is a consistent estimator of \\rho_1 are as follows :\n\nThe returns \\{r_t\\} is an independent and identically distributed sequence\n\\mathbb{E}(r_t^2) < \\infty\n\nThen \\hat\\rho_1 is asymptotically normal with mean zero and variance 1/T\nTesting\nLet H_0 : \\rho_1 = 0 be the null hypothesis versus H_a : \\rho_1 \\neq 0 the alternative hypothesis. The test statistic is the usual t ratio (check the statistics section for further description) , which is \\sqrt{T}\\hat\\rho_1 and follows asymptotically the standard normal distribution. The null hypothesis H_0 is rejected if the t ratio is large in magnitude or equivalently , the p value of the t ratio is small , lets say less than 0.05"
  },
  {
    "objectID": "docu/SEA/Q_F/statistics.html#title-statistics",
    "href": "docu/SEA/Q_F/statistics.html#title-statistics",
    "title": "Krishnakant A",
    "section": "title: Statistics",
    "text": "title: Statistics"
  },
  {
    "objectID": "docu/SEA/Q_F/statistics.html#introduction",
    "href": "docu/SEA/Q_F/statistics.html#introduction",
    "title": "Krishnakant A",
    "section": "Introduction",
    "text": "Introduction"
  },
  {
    "objectID": "docu/SEA/Q_F/statistics.html#model-limitations",
    "href": "docu/SEA/Q_F/statistics.html#model-limitations",
    "title": "Krishnakant A",
    "section": "Model limitations",
    "text": "Model limitations\n\nNormal Distribution\n\nasd"
  },
  {
    "objectID": "docu/SEA/Q_F/statistics.html#distributions",
    "href": "docu/SEA/Q_F/statistics.html#distributions",
    "title": "Statistics",
    "section": "Distributions",
    "text": "Distributions\n\nNormal Distribution\nA random variable \\(X\\) is said to be normally distrbuted if it has a probability density function as follows\n\\[f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-{\\frac{1}{2}}(\\frac{x-\\mu}{\\sigma})^2}\\]\nIt is a continous probability distribution\n\\(\\mu\\) and \\(\\sigma\\) are the mean and variance of the distribution respectively\nThe case where \\(\\mu =0\\) and \\(\\sigma = 1\\) is called standard normal distribution and its PDF is given by \\[  f(x) =\\frac{1}{\\sqrt{2\\pi}}e^{\\frac{-x^2}{2}}\\]\n\n\nLog Normal Distibution\nA random Variable \\(X\\) is said to have log normal distibution if \\(Y = \\ln{X}\\) and \\(Y\\) is normally distributed.\nThe PDF of log normal distribution is given by\n\\[f(x) = \\frac{1}{x\\sigma\\sqrt{2\\pi}}e^{(-\\frac{(\\ln{x} -\\mu)^2}{2{\\sigma}^2})}\\] where \\(\\mu\\) and \\(\\sigma\\) are the mean and variance of \\(Y(\\ln X)\\) respectively.\nHence the mean \\(\\mu^*\\) and variance \\(\\sigma^*\\) of X are as follows\n\\[\\mu^* = e^{\\mu + \\frac{1}{2}\\sigma^2}\\] \\[\\sigma^* = e^{2\\mu + 2\\sigma^2} - e^{2\\mu +\\sigma^2}\\] Important thing to note here is that \\(x\\) can take values in \\((0,\\infty)\\) only."
  },
  {
    "objectID": "docu/SEA/Q_F/statistics.html#standard-definitions",
    "href": "docu/SEA/Q_F/statistics.html#standard-definitions",
    "title": "Statistics",
    "section": "Standard Definitions",
    "text": "Standard Definitions\nExpected Value \\(\\mathbb{E}[X]\\) is given by\n\\[\\mathbb{E}[X] = \\int_{-\\infty}^{\\infty}x f(x) dx\\] Variance\\(Var(X)\\) is given by\n\\[Var(X) = \\mathbb{E}(X^2) - \\mathbb{E}{{(X)}^2}\\] \\[Var(X) = \\int_{-\\infty}^{\\infty}(x-\\mathbb{E}[X])^2 f_X(x) dx\\] Higher Moments \\(\\mathbb{E}(X^n)\\) is given by\n\\[\\mathbb{E}(X^n) = \\int_{-\\infty}^{\\infty}x^n f_X(x) dx \\] Characteristic function(CHF) \\(\\phi_X(u)\\) for \\(u \\in \\mathbb{R}\\) is given by\n\\[\\phi_X(u) = \\mathbb{E}[e^{iuX}] = \\int_{-\\infty}^{\\infty}e^{iuX}f(x)dx \\]\nMoment generating function\\(\\mathcal{M}_X(u)\\) is given by \\[\\mathcal{M}_X(u) = \\phi_X(-iu)= \\mathbb{E}[e^{uX}] = \\int_{-\\infty}^{\\infty}e^{ux}f(x)dx \\] Cumulant characteristic function \\(\\zeta_X(u)\\) is given by \\[\\zeta_X(u) = log\\mathbb{E}[e^{iux}] = log\\phi_X(u)\\]\nCentral moments$ _l$ is given by \\(\\mathbb{E}[(X-\\mu)^l]\\)\nSkewness \\(S(x)\\) and Kurtosis \\(K(x)\\) are the normalised \\(3^{rd}\\) and \\(4^{th}\\) central moments of a distribution respectively. The normalization factors are \\(\\sigma^3\\) and \\(\\sigma^4\\) respectively where \\(\\sigma\\) is the standard deviation of X.\nThe quantity \\(K(x) - 3\\) is called the excess kurtosis since \\(K(x) = 3\\) is the kurtosis for a normal distribution.\nLet \\(\\{x_1,x_2,x_3 ....x_T\\}\\) be a random sample of X with T observations \nSample Mean\\(\\hat\\mu_x\\) is given by \\[\\frac{\\sum_{t=1}^Tx_t}{T}\\] Sample Variance\\(\\hat\\sigma_x\\) is given by \\[\\frac{\\sum_{t=1}^T(x_t - \\hat\\mu_x)^2}{T-1}\\] Sample Skewness\\(\\hat S_x\\) is given by \\[\\frac{\\sum_{t=1}^T(x_t - \\hat\\mu_x)^3}{(T-1)\\hat\\sigma_x^3}\\] Sample Kurtosis\\(\\hat K_x\\) is given by \\[\\frac{\\sum_{t=1}^T(x_t - \\hat\\mu_x)^4}{(T-1)\\hat\\sigma_x^4}\\]"
  },
  {
    "objectID": "docu/SEA/Q_F/statistics.html#univaiate-distributions",
    "href": "docu/SEA/Q_F/statistics.html#univaiate-distributions",
    "title": "Statistics",
    "section": "Univaiate Distributions",
    "text": "Univaiate Distributions\n\nNormal Distribution\nA random variable \\(X\\) is said to be normally distrbuted if it has a probability density function as follows\n\\[f_X(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-{\\frac{1}{2}}(\\frac{x-\\mu}{\\sigma})^2}\\]\nIt is a continous probability distribution\n\\(\\mu\\) and \\(\\sigma\\) are the mean and variance of the distribution respectively\nThe case where \\(\\mu =0\\) and \\(\\sigma = 1\\) is called standard normal distribution and its PDF is given by \\[  f_X(x) =\\frac{1}{\\sqrt{2\\pi}}e^{\\frac{-x^2}{2}}\\]\n\nimport numpy as np\nimport math \nimport matplotlib.pyplot as plt\nimport scipy.stats as st\nfrom mpl_toolkits import mplot3d\n\n\ndef plotNormalPDF_CDF_CHF(mu ,sigma):\n    i = complex(0,1)\n    chf = lambda u : np.exp(i*mu*u -(sigma**2)*u*u/2)\n    pdf = lambda x : st.norm.pdf(x,mu,sigma)\n    cdf = lambda x : st.norm.cdf(x,mu,sigma)\n    \n    x = np.linspace(5,15,100)\n    u = np.linspace(0,5,250)\n    print(type(pdf))\n    # figure 1 ,PDF\n    plt.figure(1)\n    plt.plot(x,pdf(x))\n    plt.grid()\n    plt.xlabel('x')\n    plt.ylabel('PDF')\n  \n    # figure 2 ,CDF\n    plt.figure(2)\n    plt.plot(x,cdf(x))\n    plt.grid()\n    plt.xlabel('x')\n    plt.ylabel('CDF')\n  \n    #  figure 3 ,CHF\n  \n    plt.figure(3)\n    ax = plt.axes(projection = '3d')\n    chfV = chf(u)\n  \n    x = np.real(chfV)\n    y = np.imag(chfV)\n    ax.plot3D(u,x,y,'red')\n    ax.view_init(30 ,-120)\n    \nplotNormalPDF_CDF_CHF(10,1)\n\n<class 'function'>\n\n\n\n\n\n\n\n\n\n\n\n\n\nLog Normal Distibution\nA random Variable \\(X\\) is said to have log normal distibution if \\(Y = \\ln{X}\\) and \\(Y\\) is normally distributed.\nThe PDF of log normal distribution is given by\n\\[f_X(x) = \\frac{1}{x\\sigma\\sqrt{2\\pi}}e^{(-\\frac{(\\ln{x} -\\mu)^2}{2{\\sigma}^2})}\\] where \\(\\mu\\) and \\(\\sigma\\) are the mean and variance of \\(Y(\\ln X)\\) respectively.\nHence the mean \\(\\mu^*\\) and variance \\(\\sigma^*\\) of X are as follows\n\\[\\mu^* = e^{\\mu + \\frac{1}{2}\\sigma^2}\\] \\[\\sigma^* = e^{2\\mu + 2\\sigma^2} - e^{2\\mu +\\sigma^2}\\] Important thing to note here is that \\(x\\) can take values in \\((0,\\infty)\\) only."
  },
  {
    "objectID": "docu/SEA/Q_F/statistics.html#multivariate-distributions",
    "href": "docu/SEA/Q_F/statistics.html#multivariate-distributions",
    "title": "Statistics",
    "section": "Multivariate Distributions",
    "text": "Multivariate Distributions\n\nCorrelation\nThe correlation coefficient between two random variables \\(X\\) and \\(Y\\) is defined as \\[ \\rho_{x,y} = \\frac{Cov(X,Y)}{\\sqrt{Var(X)Var(Y)}} = \\frac{E[(X-\\mu_x)(Y-\\mu_y)]}{\\sqrt{E(X-\\mu_x)^2E(Y-\\mu_y)^2}}\\]\nThe sample correlation is given by \\[ \\hat\\rho_{x,y} = \\frac{\\sum_{t=1}^{T}(x_t - \\bar{x})(y_t - \\bar{y})}{\\sqrt{\\sum_{t=1}^T(x_t - \\bar{x})\\sum_{t=1}^T(y_t - \\bar{y})}}\\]\nTwo-dimensional densities.\nThe joint CDF of two random variables ,\\(X\\) and \\(Y\\) ,is the function \\(F_{X,Y}(.,.):\\mathbb{R}^2 \\rightarrow [0,1]\\),which is defined by:\n\\[ F_{X,Y}(x,y) = \\mathbb{P}[X\\leq{x},Y\\leq{y}]\\] If \\(X\\) and \\(Y\\) are continous variables, then the joint PDF of X and Y is a function of \\[f_{X,Y}(x,y) = \\frac{\\partial^2{F_{X,Y}(x,y)}}{\\partial{x}\\partial{y}} \\] Bivariate Normal density functions\n\\(X = [X,Y]^T\\) and \\[X \\sim \\mathcal{N}(\\begin{bmatrix}0\\\\0\\end{bmatrix},\\begin{bmatrix}1 , \\rho \\\\ \\rho ,1\\end{bmatrix}) \\]\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n#from matplotlib.mlab import bivariate_normal bivariate_normal seems to be deprecated\n\ndef bivariate_normal(X, Y, sigmax=1.0, sigmay=1.0,\n                     mux=0.0, muy=0.0, sigmaxy=0.0):\n    \"\"\"\n    Bivariate Gaussian distribution for equal shape *X*, *Y*.\n    See `bivariate normal\n    <http://mathworld.wolfram.com/BivariateNormalDistribution.html>`_\n    at mathworld.\n    \"\"\"\n    Xmu = X-mux\n    Ymu = Y-muy\n\n    rho = sigmaxy/(sigmax*sigmay)\n    z = Xmu**2/sigmax**2 + Ymu**2/sigmay**2 - 2*rho*Xmu*Ymu/(sigmax*sigmay)\n    denom = 2*np.pi*sigmax*sigmay*np.sqrt(1-rho**2)\n    return np.exp(-z/(2*(1-rho**2))) / denom\n\ndef BivariateNormalPDFPlot():\n  # Number of points in each direction\n      n = 40;\n      \n      # parameters\n      mu_1 = 0;\n      mu_2 = 0;\n      sigma_1=1;\n      sigma_2=0.5;\n      rho1=0.0\n      rho2=-0.8\n      rho3=0.8\n      \n      x = np.linspace(-3.0,3.0,n)\n      y = np.linspace(-3.0,3.0,n)\n      X,Y =np.meshgrid(x,y)\n      Z = lambda rho:bivariate_normal(X,Y,sigma_1,sigma_2,mu_1,mu_2,rho*sigma_1*sigma_2)\n      \n      fig =plt.figure(1)\n      ax = fig.add_subplot(projection= '3d')\n      ax.plot_surface(X, Y, Z(rho1),cmap='viridis',linewidth=0)\n      ax.set_xlabel('X axis')\n      ax.set_ylabel('Y axis')\n      ax.set_zlabel('Z axis')\n      plt.show()\n      \n      fig =plt.figure(2)\n      ax = fig.add_subplot(projection= '3d')\n      ax.plot_surface(X, Y, Z(rho2),cmap='viridis',linewidth=0)\n      ax.set_xlabel('X axis')\n      ax.set_ylabel('Y axis')\n      ax.set_zlabel('Z axis')\n      plt.show()\n      \n      fig =plt.figure(3)\n      ax = fig.add_subplot(projection= '3d')\n      ax.plot_surface(X, Y, Z(rho3),cmap='viridis',linewidth=0)\n      ax.set_xlabel('X axis')\n      ax.set_ylabel('Y axis')\n      ax.set_zlabel('Z axis')\n      plt.show()\n  \nBivariateNormalPDFPlot()"
  },
  {
    "objectID": "docu/SEA/data_analysis.html",
    "href": "docu/SEA/data_analysis.html",
    "title": "Data Analysis for Python",
    "section": "",
    "text": "List of important libraries and tools that are mainly used for data analysis in python * Numpy: Numerical Python has great array processing capabilities. Mostly used to pass data to algorithims from datasets/libraries. It gives the ability to perform linear algebra operations ,Fourier transforms with ease.\n\nPandas: Mostly used to manipulate and create new data structures.\nMatplotlib: Used for plots and other two dimensional data visualizations\nJupyter: Commonly used for testing , running iterations, debugging before sending off the code to production. Jupyter notebook supports R , python and julia programming languages as well as markdown for documenting the files\nSciPy: Scientific computing library. Provides functionality to integrate , optimise , signal processing,etc…\nScikit-learn: The library has machine learning models which can be used out of the box. Has all the standard machine learning models.\n\nAll of the above tools and libraries are very well documented and navigating through them is relatively easy.\n\n#Importing Libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels as sm"
  },
  {
    "objectID": "docu/SEA/data_analysis.html#python",
    "href": "docu/SEA/data_analysis.html#python",
    "title": "Data Analysis for Python",
    "section": "Python",
    "text": "Python\n\nIpython Shortcuts\n\n%a: Commonly known as magic functions. Specific to Ipython kernel\n%run a.py: runs the python file a.py located in the same working directory\n%quickref: gives quick reference\nhelp - help function for Ipython\na?: gives general information about the object a\n%load a.py: Import a.py script into the code cell\n%paste: Will paste contents in the keyboard and execute the as a single block\n%timeit b:Gives the execution time of python statement b.Can be run muliple times giving the average time over all iterations as output.\n%debug: Various positional arguments available to debug a python statement.\n%pwd: Gives current path as output\n%xdel variable : Deletes teh varaible and attempt to clear all references to the object in python\n\n\n\nPython\n\n#checks the current versiom of kernel being used\nimport sys\nprint(sys.executable)\nprint(sys.version)\nprint(sys.version_info)\n\n/opt/homebrew/opt/python@3.10/bin/python3.10\n3.10.9 (main, Dec 15 2022, 17:11:09) [Clang 14.0.0 (clang-1400.0.29.202)]\nsys.version_info(major=3, minor=10, micro=9, releaselevel='final', serial=0)\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(subplot_kw={'projection': 'polar'})\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()"
  },
  {
    "objectID": "docu/Resources/Research_papers.html#compuational-finance",
    "href": "docu/Resources/Research_papers.html#compuational-finance",
    "title": "Reseach Papers",
    "section": "Compuational finance",
    "text": "Compuational finance"
  },
  {
    "objectID": "docu/Resources/Research_papers.html#decentralised-finance-anb-cryptography-papers",
    "href": "docu/Resources/Research_papers.html#decentralised-finance-anb-cryptography-papers",
    "title": "Reseach Papers",
    "section": "Decentralised finance anb Cryptography papers",
    "text": "Decentralised finance anb Cryptography papers"
  },
  {
    "objectID": "docu/Resources/Research_papers.html#abstract-mathematics",
    "href": "docu/Resources/Research_papers.html#abstract-mathematics",
    "title": "Reseach Papers",
    "section": "Abstract Mathematics",
    "text": "Abstract Mathematics"
  },
  {
    "objectID": "docu/Resources/Research_papers.html#decentralised-finance-and-cryptography-papers",
    "href": "docu/Resources/Research_papers.html#decentralised-finance-and-cryptography-papers",
    "title": "Reseach Papers",
    "section": "Decentralised finance and Cryptography papers",
    "text": "Decentralised finance and Cryptography papers\n\nTowards a Theory of Maximal Extractable Value I: Constant Function Market Makers by Kshitij Kulkarni ,et.al\nAn analysis of Uniswap markets\nReplicating market makers\nOptimal fees for geometric mean market makers\nImproved price oracles: Constant function market makers\nConstant function market makers: Multi-asset trades via convex optimization\nBitcoin, Currencies, and Fragility by Taleb\nBeige Paper\nETHEREUM: A SECURE DECENTRALISED GENERALISED TRANSACTION LEDGER\nA primer on perpetuals"
  },
  {
    "objectID": "docu/SEA/StochC.html#preliminaries",
    "href": "docu/SEA/StochC.html#preliminaries",
    "title": "Stochastic Calculus",
    "section": "Preliminaries",
    "text": "Preliminaries\n\nDefinition 1 (\\sigma algebra) : Let \\Omega be a nonempty set , and let \\mathcal{F} be a set of collections of subsets of \\Omega. We say that \\mathcal{F} is a \\sigma algebra provided that\n\nThe empty set \\emptyset belongs to \\mathcal{F}\nA and A^{c} belong to \\mathcal{F} whenever A \\in \\mathcal{F}\nwhenever A_1 ,A_2 ,A_3 ..... \\in \\mathcal{F} then \\bigcup_{i=1}^\\infty A_{i} \\in \\mathcal{F}\n\n\n \n\nDefinition 2 (Probability measure) : Let \\Omega be a nonempty set , and let \\mathbb{F} be \\sigma algebra of subsets of \\Omega. A probability measure \\mathbb{P} is a function that, to every set A \\in \\mathcal{F} , assigns a number in [0,1] called the probability of A written as \\mathbb{P}(A) .We require\n\n\\mathbb{P}(\\Omega) = 1 and\n(Countable Additivity) Whenever A_1,A_2,A_3.... is a sequence of disjoint sets in \\mathcal{F} , then \\mathbb{P}(\\bigcup_{n=1}^\\infty A_n) = \\sum_{n=1}^{\\infty} \\mathbb{P}(A_n) \n\n The Triple (\\Omega, \\mathcal{F},\\mathbb{}P) is called probability measure\n\n From the above it follows that for finitely many disjoint sets A_1, A_2,A_3....A_n \\mathbb{P}(\\bigcup_{n=1}^N) = \\sum_{n=1}^N \\mathbb{P}(A_n)\nand \\mathbb{P}(A^c) = 1 - \\mathbb{P}(A)\n\nExample 1 (Uniform(Lebesgue) measure on [0,1])  \\Omega = [0,1] \\mathbb{P}[a,b] = b-a ,0 \\leq a \\leq b \\leq1  single points have zero probability and  (a,b) can be written as  \\bigcup_{n=1}^\\infty[a+\\frac{1}{n} , b -\\frac{1}{n}]\n\n  The \\sigma-algebra beginning with closed intervals and adding everything else necessary in order to have a \\sigma- algebra is called a Borel \\sigma-algebra and denoted by \\mathcal{B}[0,1]. Borel \\sigma-algebras are \\sigma-algebra generated by open sets and equivalently closed sets over any topological space.\n\nDefinition 3 let (\\Omega,\\mathcal{F},\\mathbb{P}) be a probability space. If a set A \\in \\mathcal{F} satisfies \\mathbb{P}(A) = 1, we say the event A occurs almost surely\n\n\nExample 2 Let \\mathbb{P} be the uniform measure as defined in example 1. Define X(\\omega) = \\omega and Y(\\omega) = 1 - \\omega for all \\omega \\in [0,1] , then the distribution measure of X is uniform \\mu_{X}[a,b] = \\mathbb{P}\\{\\omega;a\\leq X(w)\\leq b\\} = \\mathbb{P}[a,b] = b-a, 0\\leq a \\leq b \\leq 1 by definition of \\mathbb{P}.  Its easy to see to see that X and Y have the same distribution. But under the probability measure\\mathbb{\\widetilde{P}} on [0,1] defined by \\mathbb{\\widetilde{P}[a,b]} = \\int_a^b 2\\omega \\, d\\omega = b^2-a^2  , 0 \\leq a\\leq b \\leq1  X and Y have different distributions.\n\n\nDefinition 4 (cumulative distribution function and density function)  F(x) = \\mathbb{P}\\{X \\leq x\\} , x \\in \\mathbb{R} \\mu_X[a,b] = \\mathbb{P}\\{a \\leq X \\leq b \\} = \\int_a^b f(x) \\, dx , -\\infty < a \\leq b < \\infty f(x) is called the density funtion and F(x) is called the cummulative distribution function.\n\n\nExample 3 (Standard normal random variable) Let \\phi(x)  = \\frac {a}{\\sqrt{2\\pi}}e^{\\frac{-x^2}{2}} be the standard normal density and definiing the cummulative normal distribution function as N(x) = \\int_{-\\infty}^x \\phi(\\xi)\\,d\\xi The function N(x) is strictly increasing function which is surjective onto (0,1) from \\mathbb{R}, so it has a strictly increasing inverse function N^{-1}(y),y \\in (0,1). Let Y be a uniformly distributed random variable defined on some probability space (\\Omega,\\mathcal{F},\\mathbb{P}) and set X = N^{-1}(Y) then\n\\begin{align*}\n\n\\mu_X[a,b] &= \\mathbb{P}\\{\\omega \\in \\Omega ; a \\leq X(\\omega) \\leq b\\} \\\\\n\n          &= \\mathbb{P}\\{\\omega \\in \\Omega ; a \\leq N^{-1}(Y(\\omega)) \\leq b\\}\\\\\n            \n          &= \\mathbb{P}\\{\\omega \\in \\Omega ; N(a) \\leq Y(\\omega) \\leq N(b) \\} \\\\\n        \n          &= N(b) - N(a) \\\\\n          &= \\int_a^b \\phi(x) \\, dx\n\n\\end{align*}\nAny random variable that has this distribution regardless of the probability space is called standard normal distribution. Thing to note the use of uniformly distributed random variable for generating a standard random variable is called probability integral transform and this is commonly used in Monte Carlo simulation"
  },
  {
    "objectID": "docu/SEA/python.html#c",
    "href": "docu/SEA/python.html#c",
    "title": "Programming language",
    "section": "C++",
    "text": "C++"
  },
  {
    "objectID": "docu/SEA/python.html#rust",
    "href": "docu/SEA/python.html#rust",
    "title": "Programming language",
    "section": "Rust",
    "text": "Rust"
  },
  {
    "objectID": "docu/SEA/python.html#r",
    "href": "docu/SEA/python.html#r",
    "title": "Programming language",
    "section": "R",
    "text": "R"
  },
  {
    "objectID": "docu/Resources/Research_papers.html#computational-finance",
    "href": "docu/Resources/Research_papers.html#computational-finance",
    "title": "Reseach Papers",
    "section": "Computational finance",
    "text": "Computational finance"
  },
  {
    "objectID": "docu/SEA/Interesting_implementation.html",
    "href": "docu/SEA/Interesting_implementation.html",
    "title": "Interesting Implementations",
    "section": "",
    "text": "Mersenne Twister\n\nimport numpy as np\n\na = np.random.randn(3,4)\nb = np.random.randn(1,4)\nc = a+b\nprint (c)\n\n[[ 0.32295651 -0.85615547  1.05162089  0.39146209]\n [-2.97129035 -1.50722391 -0.04004183  0.13532617]\n [ 1.57507404 -3.54460121 -0.21055781  0.47160234]]"
  },
  {
    "objectID": "docu/SEA/Machine learning/tools and frameworks .html",
    "href": "docu/SEA/Machine learning/tools and frameworks .html",
    "title": "Tools and frameworks",
    "section": "",
    "text": "import tensorflow as tf\nimport h5py\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom tensorflow.python.framework.ops import EagerTensor\nfrom tensorflow.python.ops.resource_variable_ops import ResourceVariable\nimport time"
  },
  {
    "objectID": "docu/Resources/resourcelist.html",
    "href": "docu/Resources/resourcelist.html",
    "title": "Books , Video lectures",
    "section": "",
    "text": "Computational Financ"
  },
  {
    "objectID": "docu/Untitled1.html",
    "href": "docu/Untitled1.html",
    "title": "Krishnakant A",
    "section": "",
    "text": "``` {.c++14 .cell-code} #include \nstd::cout << “main” ;\n\n::: {.cell-output .cell-output-stderr}\nIn file included from input_line_5:1: In file included from /Users/krishnakantammanamanachi/opt/anaconda3/envs/cling/include/xeus/xinterpreter.hpp:17: In file included from /Users/krishnakantammanamanachi/opt/anaconda3/envs/cling/include/xeus/xcomm.hpp:19: In file included from /Users/krishnakantammanamanachi/opt/anaconda3/envs/cling/include/nlohmann/json.hpp:55: In file included from /Users/krishnakantammanamanachi/opt/anaconda3/envs/cling/include/nlohmann/detail/input/binary_reader.hpp:16: In file included from /Users/krishnakantammanamanachi/opt/anaconda3/envs/cling/include/nlohmann/detail/input/input_adapters.hpp:7: In file included from /Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/c++/v1/istream:163: /Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/c++/v1/ostream:725:39: error: no member named ‘fill’ in ’std::__1::basic_ostream’ __os.fill()).failed()) ~~~~ ^ /Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/c++/v1/ostream:857:19: note: in instantiation of function template specialization ’std::__1::__put_character_sequence<char, std::__1::char_traits >’ requested here return _VSTD::__put_character_sequence(__os, __str, _Traits::length(__str)); ^ input_line_27:2:12: note: in instantiation of function template specialization ’std::__1::operator<<<std::__1::char_traits >’ requested here std::cout << “main” ; ^\n:::\n\n::: {.cell-output .cell-output-error}\nInterpreter Error: ```\n\n:::"
  },
  {
    "objectID": "docu/Untitled.html",
    "href": "docu/Untitled.html",
    "title": "Krishnakant A",
    "section": "",
    "text": "{.c++14 .cell-code} #include iostream"
  },
  {
    "objectID": "docu/SEA/Machine learning/Untitled.html",
    "href": "docu/SEA/Machine learning/Untitled.html",
    "title": "Hands-On Machine Learning with Scikit-Learn ,Keras and Tensorflow",
    "section": "",
    "text": "A pure code based implementation with necessary mathematical formulations included.\n\n\n\n\n\n Root Mean Square Error  RMSE(\\textbf{X},h)= \\sqrt{\\frac{\\sum_{i = 1}^{m}{(h{(x^{(i)})- y ^{(i)}}})^2}{m}} $ m - $ is the number of instances in the datasets you are measuring the RMSE on  $x^{(i)} - $ is the vector of all teh feature values of the i^{th} instance in the dataset $y^{(i)} - $ is its label $ - $ is the matrix containing all the feature value of all the instances in the dataset. x^{(i)} transpose is the i^{th} row in \\textbf{X}  h - is the system’s prediction function also called as hypothesis. It is also represented by \\hat{y} where \\hat{y}^{(i)} = h(x^{(i)}) $ RMSE(,h) - $ is the cost function measured on the set of examples using the hypothesis h \n Mean absolute Error  MAE(\\textbf{X},h)= \\frac{\\sum_{i = 1}^{m}{|h{(x^{(i)})- y ^{(i)}}}|}{m}\nThe difference between these two performance measure is how the norm is caculated. RSME makes use of \\mathcal{l}_2 norm while MAE makes use of \\mathcal{l}_1 norm.\n\n\n\n\nimport os \nimport tarfile\nimport urllib\n\nDownload_Root = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\nHousing_Path = os.path.join(\"Data\",\"housing\") # for creating your local directory with DATA as main directory and housign as sub directory\nHousing_URL = Download_Root + \"datasets/housing/housing.tgz\"\n\ndef fetch_housing_data(housing_url = Housing_URL,housing_path = Housing_Path):\n    os.makedirs(housing_path,exist_ok = True)# Will create a directory with name at housing path , exist_ok will check if it already exist , if yes leaving it unaltered\n    tgz_path = os.path.join(housing_path,\"housing.tgz\")\n    urllib.request.urlretrieve(housing_url,tgz_path)\n    housing_tgz = tarfile.open(tgz_path)\n    housing_tgz.extractall(path=housing_path)\n    housing_tgz.close()\n    \nfetch_housing_data() # call the function\n\n\nimport pandas as pd\n\ndef load_housing_data(housing_path = Housing_Path):\n    csv_path = os.path.join(housing_path,\"housing.csv\")\n    return pd.read_csv(csv_path)\n\n\nhousing = load_housing_data()\nhousing.head()\n\n\n\n\n\n  \n    \n      \n      longitude\n      latitude\n      housing_median_age\n      total_rooms\n      total_bedrooms\n      population\n      households\n      median_income\n      median_house_value\n      ocean_proximity\n    \n  \n  \n    \n      0\n      -122.23\n      37.88\n      41.0\n      880.0\n      129.0\n      322.0\n      126.0\n      8.3252\n      452600.0\n      NEAR BAY\n    \n    \n      1\n      -122.22\n      37.86\n      21.0\n      7099.0\n      1106.0\n      2401.0\n      1138.0\n      8.3014\n      358500.0\n      NEAR BAY\n    \n    \n      2\n      -122.24\n      37.85\n      52.0\n      1467.0\n      190.0\n      496.0\n      177.0\n      7.2574\n      352100.0\n      NEAR BAY\n    \n    \n      3\n      -122.25\n      37.85\n      52.0\n      1274.0\n      235.0\n      558.0\n      219.0\n      5.6431\n      341300.0\n      NEAR BAY\n    \n    \n      4\n      -122.25\n      37.85\n      52.0\n      1627.0\n      280.0\n      565.0\n      259.0\n      3.8462\n      342200.0\n      NEAR BAY\n    \n  \n\n\n\n\n\nhousing.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 20640 entries, 0 to 20639\nData columns (total 10 columns):\n #   Column              Non-Null Count  Dtype  \n---  ------              --------------  -----  \n 0   longitude           20640 non-null  float64\n 1   latitude            20640 non-null  float64\n 2   housing_median_age  20640 non-null  float64\n 3   total_rooms         20640 non-null  float64\n 4   total_bedrooms      20433 non-null  float64\n 5   population          20640 non-null  float64\n 6   households          20640 non-null  float64\n 7   median_income       20640 non-null  float64\n 8   median_house_value  20640 non-null  float64\n 9   ocean_proximity     20640 non-null  object \ndtypes: float64(9), object(1)\nmemory usage: 1.6+ MB\n\n\n\nhousing[\"ocean_proximity\"].value_counts()\n\n<1H OCEAN     9136\nINLAND        6551\nNEAR OCEAN    2658\nNEAR BAY      2290\nISLAND           5\nName: ocean_proximity, dtype: int64\n\n\n\nhousing.describe()\n\n\n\n\n\n  \n    \n      \n      longitude\n      latitude\n      housing_median_age\n      total_rooms\n      total_bedrooms\n      population\n      households\n      median_income\n      median_house_value\n    \n  \n  \n    \n      count\n      20640.000000\n      20640.000000\n      20640.000000\n      20640.000000\n      20433.000000\n      20640.000000\n      20640.000000\n      20640.000000\n      20640.000000\n    \n    \n      mean\n      -119.569704\n      35.631861\n      28.639486\n      2635.763081\n      537.870553\n      1425.476744\n      499.539680\n      3.870671\n      206855.816909\n    \n    \n      std\n      2.003532\n      2.135952\n      12.585558\n      2181.615252\n      421.385070\n      1132.462122\n      382.329753\n      1.899822\n      115395.615874\n    \n    \n      min\n      -124.350000\n      32.540000\n      1.000000\n      2.000000\n      1.000000\n      3.000000\n      1.000000\n      0.499900\n      14999.000000\n    \n    \n      25%\n      -121.800000\n      33.930000\n      18.000000\n      1447.750000\n      296.000000\n      787.000000\n      280.000000\n      2.563400\n      119600.000000\n    \n    \n      50%\n      -118.490000\n      34.260000\n      29.000000\n      2127.000000\n      435.000000\n      1166.000000\n      409.000000\n      3.534800\n      179700.000000\n    \n    \n      75%\n      -118.010000\n      37.710000\n      37.000000\n      3148.000000\n      647.000000\n      1725.000000\n      605.000000\n      4.743250\n      264725.000000\n    \n    \n      max\n      -114.310000\n      41.950000\n      52.000000\n      39320.000000\n      6445.000000\n      35682.000000\n      6082.000000\n      15.000100\n      500001.000000\n    \n  \n\n\n\n\n\n%matplotlib inline \nimport matplotlib.pyplot as plt\nhousing.hist(bins = 50 ,figsize = (20,15))\n\narray([[<AxesSubplot: title={'center': 'longitude'}>,\n        <AxesSubplot: title={'center': 'latitude'}>,\n        <AxesSubplot: title={'center': 'housing_median_age'}>],\n       [<AxesSubplot: title={'center': 'total_rooms'}>,\n        <AxesSubplot: title={'center': 'total_bedrooms'}>,\n        <AxesSubplot: title={'center': 'population'}>],\n       [<AxesSubplot: title={'center': 'households'}>,\n        <AxesSubplot: title={'center': 'median_income'}>,\n        <AxesSubplot: title={'center': 'median_house_value'}>]],\n      dtype=object)\n\n\n\n\n\nFew point about the data\n\nThe median income is capped at 15 for higher median incomes and at 0.5 for lower median incomes. The numbers are denominated in $10000 i.e 3 in the chart represents 30,000 USD\nThe housing median age and median house value were also capped. Since median house value is our target variable this is a source of a problem.The prediction might never go beyond the capped limit.\nThe attributes have different scales\nMany of the histograms are tail-heavy. Important to normalize the data as some machine learning algorithims might fail to detect patterns effectively\n\n\n\n\n\nimport numpy as np\nnp.random.seed(50)# will always give the same test set and train set \ndef split_train_test(data,test_ratio):\n    shuffled_indices = np.random.permutation(len(data))\n    test_set_size = int(len(data)*test_ratio)\n    test_indices = shuffled_indices[:test_set_size]\n    train_indices = shuffled_indices[test_set_size:]\n    return data.loc[train_indices] ,data.loc[test_indices]\n\ntrain_set , test_set = split_train_test(housing,0.2)\nprint(len(train_set))\nprint(len(test_set))\n\n16512\n4128"
  },
  {
    "objectID": "docu/SEA/Machine learning/handsonml.html",
    "href": "docu/SEA/Machine learning/handsonml.html",
    "title": "Hands-On Machine Learning with Scikit-Learn ,Keras and Tensorflow",
    "section": "",
    "text": "A pure code based implementation with necessary mathematical formulations included.\n\n\n\n\n\n Root Mean Square Error  RMSE(\\textbf{X},h)= \\sqrt{\\frac{\\sum_{i = 1}^{m}{(h{(x^{(i)})- y ^{(i)}}})^2}{m}} $ m - $ is the number of instances in the datasets you are measuring the RMSE on  $x^{(i)} - $ is the vector of all teh feature values of the i^{th} instance in the dataset $y^{(i)} - $ is its label $ - $ is the matrix containing all the feature value of all the instances in the dataset. x^{(i)} transpose is the i^{th} row in \\textbf{X}  h - is the system’s prediction function also called as hypothesis. It is also represented by \\hat{y} where \\hat{y}^{(i)} = h(x^{(i)}) $ RMSE(,h) - $ is the cost function measured on the set of examples using the hypothesis h \n Mean absolute Error  MAE(\\textbf{X},h)= \\frac{\\sum_{i = 1}^{m}{|h{(x^{(i)})- y ^{(i)}}}|}{m}\nThe difference between these two performance measure is how the norm is caculated. RSME makes use of \\mathcal{l}_2 norm while MAE makes use of \\mathcal{l}_1 norm.\n\n\n\n\nimport os \nimport tarfile\nimport urllib\n\nDownload_Root = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\nHousing_Path = os.path.join(\"Data\",\"housing\") # for creating your local directory with DATA as main directory and housign as sub directory\nHousing_URL = Download_Root + \"datasets/housing/housing.tgz\"\n\ndef fetch_housing_data(housing_url = Housing_URL,housing_path = Housing_Path):\n    os.makedirs(housing_path,exist_ok = True)# Will create a directory with name at housing path , exist_ok will check if it already exist , if yes leaving it unaltered\n    tgz_path = os.path.join(housing_path,\"housing.tgz\")\n    urllib.request.urlretrieve(housing_url,tgz_path)\n    housing_tgz = tarfile.open(tgz_path)\n    housing_tgz.extractall(path=housing_path)\n    housing_tgz.close()\n    \nfetch_housing_data() # call the function\n\n\nimport pandas as pd\n\ndef load_housing_data(housing_path = Housing_Path):\n    csv_path = os.path.join(housing_path,\"housing.csv\")\n    return pd.read_csv(csv_path)\n\n\nhousing = load_housing_data()\nhousing.head()\n\n\n\n\n\n  \n    \n      \n      longitude\n      latitude\n      housing_median_age\n      total_rooms\n      total_bedrooms\n      population\n      households\n      median_income\n      median_house_value\n      ocean_proximity\n    \n  \n  \n    \n      0\n      -122.23\n      37.88\n      41.0\n      880.0\n      129.0\n      322.0\n      126.0\n      8.3252\n      452600.0\n      NEAR BAY\n    \n    \n      1\n      -122.22\n      37.86\n      21.0\n      7099.0\n      1106.0\n      2401.0\n      1138.0\n      8.3014\n      358500.0\n      NEAR BAY\n    \n    \n      2\n      -122.24\n      37.85\n      52.0\n      1467.0\n      190.0\n      496.0\n      177.0\n      7.2574\n      352100.0\n      NEAR BAY\n    \n    \n      3\n      -122.25\n      37.85\n      52.0\n      1274.0\n      235.0\n      558.0\n      219.0\n      5.6431\n      341300.0\n      NEAR BAY\n    \n    \n      4\n      -122.25\n      37.85\n      52.0\n      1627.0\n      280.0\n      565.0\n      259.0\n      3.8462\n      342200.0\n      NEAR BAY\n    \n  \n\n\n\n\n\nhousing.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 20640 entries, 0 to 20639\nData columns (total 10 columns):\n #   Column              Non-Null Count  Dtype  \n---  ------              --------------  -----  \n 0   longitude           20640 non-null  float64\n 1   latitude            20640 non-null  float64\n 2   housing_median_age  20640 non-null  float64\n 3   total_rooms         20640 non-null  float64\n 4   total_bedrooms      20433 non-null  float64\n 5   population          20640 non-null  float64\n 6   households          20640 non-null  float64\n 7   median_income       20640 non-null  float64\n 8   median_house_value  20640 non-null  float64\n 9   ocean_proximity     20640 non-null  object \ndtypes: float64(9), object(1)\nmemory usage: 1.6+ MB\n\n\n\nhousing[\"ocean_proximity\"].value_counts()\n\n<1H OCEAN     9136\nINLAND        6551\nNEAR OCEAN    2658\nNEAR BAY      2290\nISLAND           5\nName: ocean_proximity, dtype: int64\n\n\n\nhousing.describe()\n\n\n\n\n\n  \n    \n      \n      longitude\n      latitude\n      housing_median_age\n      total_rooms\n      total_bedrooms\n      population\n      households\n      median_income\n      median_house_value\n    \n  \n  \n    \n      count\n      20640.000000\n      20640.000000\n      20640.000000\n      20640.000000\n      20433.000000\n      20640.000000\n      20640.000000\n      20640.000000\n      20640.000000\n    \n    \n      mean\n      -119.569704\n      35.631861\n      28.639486\n      2635.763081\n      537.870553\n      1425.476744\n      499.539680\n      3.870671\n      206855.816909\n    \n    \n      std\n      2.003532\n      2.135952\n      12.585558\n      2181.615252\n      421.385070\n      1132.462122\n      382.329753\n      1.899822\n      115395.615874\n    \n    \n      min\n      -124.350000\n      32.540000\n      1.000000\n      2.000000\n      1.000000\n      3.000000\n      1.000000\n      0.499900\n      14999.000000\n    \n    \n      25%\n      -121.800000\n      33.930000\n      18.000000\n      1447.750000\n      296.000000\n      787.000000\n      280.000000\n      2.563400\n      119600.000000\n    \n    \n      50%\n      -118.490000\n      34.260000\n      29.000000\n      2127.000000\n      435.000000\n      1166.000000\n      409.000000\n      3.534800\n      179700.000000\n    \n    \n      75%\n      -118.010000\n      37.710000\n      37.000000\n      3148.000000\n      647.000000\n      1725.000000\n      605.000000\n      4.743250\n      264725.000000\n    \n    \n      max\n      -114.310000\n      41.950000\n      52.000000\n      39320.000000\n      6445.000000\n      35682.000000\n      6082.000000\n      15.000100\n      500001.000000\n    \n  \n\n\n\n\n\n%matplotlib inline \nimport matplotlib.pyplot as plt\nhousing.hist(bins = 50 ,figsize = (20,15))\n\narray([[<AxesSubplot: title={'center': 'longitude'}>,\n        <AxesSubplot: title={'center': 'latitude'}>,\n        <AxesSubplot: title={'center': 'housing_median_age'}>],\n       [<AxesSubplot: title={'center': 'total_rooms'}>,\n        <AxesSubplot: title={'center': 'total_bedrooms'}>,\n        <AxesSubplot: title={'center': 'population'}>],\n       [<AxesSubplot: title={'center': 'households'}>,\n        <AxesSubplot: title={'center': 'median_income'}>,\n        <AxesSubplot: title={'center': 'median_house_value'}>]],\n      dtype=object)\n\n\n\n\n\nFew point about the data\n\nThe median income is capped at 15 for higher median incomes and at 0.5 for lower median incomes. The numbers are denominated in $10000 i.e 3 in the chart represents 30,000 USD\nThe housing median age and median house value were also capped. Since median house value is our target variable this is a source of a problem.The prediction might never go beyond the capped limit.\nThe attributes have different scales\nMany of the histograms are tail-heavy. Important to normalize the data as some machine learning algorithims might fail to detect patterns effectively\n\n\n\n\n\nimport numpy as np\nnp.random.seed(50)# will always give the same test set and train set \ndef split_train_test(data,test_ratio):\n    shuffled_indices = np.random.permutation(len(data))\n    test_set_size = int(len(data)*test_ratio)\n    test_indices = shuffled_indices[:test_set_size]\n    train_indices = shuffled_indices[test_set_size:]\n    return data.loc[train_indices] ,data.loc[test_indices]\n\ntrain_set , test_set = split_train_test(housing,0.2)\nprint(len(train_set))\nprint(len(test_set))\n\n16512\n4128"
  },
  {
    "objectID": "docu/SEA/Q_F/statistics.html#hypothesis-testing",
    "href": "docu/SEA/Q_F/statistics.html#hypothesis-testing",
    "title": "Statistics",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\n t-statistic  is the ratio of departure of the estimated value of a paramater from its hypothesized value to it’s standard error.\nIt is used when the sample size is small or the population standard deviation is unknown.\nLet \\(\\hat\\beta\\) be an estimator of parameter \\(\\beta\\) in some statistical model. Then the t-statistic is given by \\[ t_{\\hat\\beta}  = \\frac{\\hat\\beta - \\beta_0}{s.e(\\hat\\beta)}\\] where \\(s.e(\\hat\\beta)\\) is the standard error of the estimator \\(\\hat\\beta\\) for \\(\\beta\\) and \\(\\beta_0\\) is a non-random , know constant , which may or maynot match actual unknow parameter value \\(\\beta\\)"
  },
  {
    "objectID": "docu/SEA/StochC.html#stochastic-processes",
    "href": "docu/SEA/StochC.html#stochastic-processes",
    "title": "Stochastic Calculus",
    "section": "Stochastic Processes",
    "text": "Stochastic Processes"
  },
  {
    "objectID": "docu/SEA/DSA.html#algorithims",
    "href": "docu/SEA/DSA.html#algorithims",
    "title": "Data Structure and algorithims",
    "section": "Algorithims",
    "text": "Algorithims"
  },
  {
    "objectID": "docu/Book_Implementations/FCSC.html",
    "href": "docu/Book_Implementations/FCSC.html",
    "title": "First Course stochastic calculus",
    "section": "",
    "text": "```{}"
  },
  {
    "objectID": "docu/SEA/Machine learning/tools and frameworks .html#pytorch",
    "href": "docu/SEA/Machine learning/tools and frameworks .html#pytorch",
    "title": "Tools and frameworks",
    "section": "PyTorch",
    "text": "PyTorch"
  },
  {
    "objectID": "docu/SEA/Q_F/statistics.html",
    "href": "docu/SEA/Q_F/statistics.html",
    "title": "Statistics",
    "section": "",
    "text": "This is majorl ## Standard Definitions\nExpected Value \\(\\mathbb{E}[X]\\) is given by\n\\[\\mathbb{E}[X] = \\int_{-\\infty}^{\\infty}x f(x) dx\\] Variance\\(Var(X)\\) is given by\n\\[Var(X) = \\mathbb{E}(X^2) - \\mathbb{E}{{(X)}^2}\\] \\[Var(X) = \\int_{-\\infty}^{\\infty}(x-\\mathbb{E}[X])^2 f_X(x) dx\\] Higher Moments \\(\\mathbb{E}(X^n)\\) is given by\n\\[\\mathbb{E}(X^n) = \\int_{-\\infty}^{\\infty}x^n f_X(x) dx \\] Characteristic function(CHF) \\(\\phi_X(u)\\) for \\(u \\in \\mathbb{R}\\) is given by\n\\[\\phi_X(u) = \\mathbb{E}[e^{iuX}] = \\int_{-\\infty}^{\\infty}e^{iuX}f(x)dx \\]\nMoment generating function\\(\\mathcal{M}_X(u)\\) is given by \\[\\mathcal{M}_X(u) = \\phi_X(-iu)= \\mathbb{E}[e^{uX}] = \\int_{-\\infty}^{\\infty}e^{ux}f(x)dx \\] Cumulant characteristic function \\(\\zeta_X(u)\\) is given by \\[\\zeta_X(u) = log\\mathbb{E}[e^{iux}] = log\\phi_X(u)\\]\nCentral moments$ _l$ is given by \\(\\mathbb{E}[(X-\\mu)^l]\\)\nSkewness \\(S(x)\\) and Kurtosis \\(K(x)\\) are the normalised \\(3^{rd}\\) and \\(4^{th}\\) central moments of a distribution respectively. The normalization factors are \\(\\sigma^3\\) and \\(\\sigma^4\\) respectively where \\(\\sigma\\) is the standard deviation of X.\nThe quantity \\(K(x) - 3\\) is called the excess kurtosis since \\(K(x) = 3\\) is the kurtosis for a normal distribution.\nLet \\(\\{x_1,x_2,x_3 ....x_T\\}\\) be a random sample of X with T observations \nSample Mean\\(\\hat\\mu_x\\) is given by \\[\\frac{\\sum_{t=1}^Tx_t}{T}\\] Sample Variance\\(\\hat\\sigma_x\\) is given by \\[\\frac{\\sum_{t=1}^T(x_t - \\hat\\mu_x)^2}{T-1}\\] Sample Skewness\\(\\hat S_x\\) is given by \\[\\frac{\\sum_{t=1}^T(x_t - \\hat\\mu_x)^3}{(T-1)\\hat\\sigma_x^3}\\] Sample Kurtosis\\(\\hat K_x\\) is given by \\[\\frac{\\sum_{t=1}^T(x_t - \\hat\\mu_x)^4}{(T-1)\\hat\\sigma_x^4}\\]"
  },
  {
    "objectID": "docu/SEA/Machine learning/KDB.html",
    "href": "docu/SEA/Machine learning/KDB.html",
    "title": "KDB",
    "section": "",
    "text": "til each 20+til 10"
  },
  {
    "objectID": "docu/Book_Implementations/DLIP.html",
    "href": "docu/Book_Implementations/DLIP.html",
    "title": "Krishnakant A",
    "section": "",
    "text": "import sys\nprint(sys.executable)\nprint(sys.version)\nprint(sys.version_info)\n\n/Users/krishnakantammanamanachi/opt/anaconda3/envs/tf/bin/python\n3.10.9 | packaged by conda-forge | (main, Feb  2 2023, 20:24:27) [Clang 14.0.6 ]\nsys.version_info(major=3, minor=10, micro=9, releaselevel='final', serial=0)\n\n\n\nfrom keras.datasets import mnist\n[(train_images,train_labels),(test_images,test_labels)] = mnist.load_data()\n\n\nprint( \"Shape of train images\",train_images.shape )\nprint( \"Shape of test images\",test_images.shape )\nprint( \"Train labels format\",train_labels )\n\nShape of train images (60000, 28, 28)\nShape of test images (10000, 28, 28)\nTrain labels format [5 0 4 ... 5 6 8]\n\n\n\n\n\nfrom keras import models , layers\n# Creating a two layer dense(all hidden nodes are connected all input nodes - fully connected) neural network.\n# activation function for each layer is specified\n# \nnetwork = models.Sequential()\nnetwork.add(layers.Dense(512,activation = 'relu',input_shape = (28*28,))) # Creating a neural network layer with 512 hidden nodes and 'relu' as activation function\nnetwork.add(layers.Dense(10,activation = 'softmax')) # creating another layer in succession to the previous layer with 10 nodes as output \n\n\n## The Compiler Step\n\nnetwork.compile(optimizer = 'rmsprop',\n                 loss = 'categorical_crossentropy',\n                  metrics = ['accuracy'])\n\n\n## Preparing the image data\n\ntrain_images = train_images.reshape((60000,28*28)) # a neural network can only take a 1-d vector as an input \ntrain_images = train_images.astype('float32')/255 # Changing the type of the input data and scaling it such that the value lies between [0,1]\n\ntest_images = test_images.reshape((10000,28*28))\ntest_images = test_images.astype('float32')/255\n\ntrain_images.shape\n\n(60000, 784)\n\n\n\n## preparing the labels \n# since our labels are categorical in nature while a neural netowrk can only understand vector representations we convert our prediction labels into\n# required format\n\nfrom keras.utils import to_categorical\n\ntrain_labels = to_categorical(train_labels)\ntest_labels  = to_categorical(test_labels)\n\n\nimport sys\nimport tensorflow.keras\nimport pandas as pd\n#import sklearn as sk\nimport scipy as sp\nimport tensorflow as tf\nimport platform\nprint(f\"Python Platform: {platform.platform()}\")\nprint(f\"Tensor Flow Version: {tf.__version__}\")\nprint(f\"Keras Version: {tensorflow.keras.__version__}\")\nprint()\nprint(f\"Python {sys.version}\")\nprint(f\"Pandas {pd.__version__}\")\n#print(f\"Scikit-Learn {sk.__version__}\")\nprint(f\"SciPy {sp.__version__}\")\ngpu = len(tf.config.list_physical_devices('GPU'))>0\nprint(\"GPU is\", \"available\" if gpu else \"NOT AVAILABLE\")\n\nPython Platform: macOS-13.1-x86_64-i386-64bit\nTensor Flow Version: 2.11.0\nKeras Version: 2.11.0\n\nPython 3.10.9 | packaged by conda-forge | (main, Feb  2 2023, 20:24:27) [Clang 14.0.6 ]\nPandas 1.5.3\nSciPy 1.10.0\nGPU is NOT AVAILABLE\n\n\n\n## Passing our input and output data to the neural network we have designed \nnetwork.fit(train_images,train_labels,epochs = 100)\n\nEpoch 1/100\n1875/1875 [==============================] - 5s 3ms/step - loss: 2.3013 - accuracy: 0.1124\nEpoch 2/100\n1875/1875 [==============================] - 5s 3ms/step - loss: 2.3013 - accuracy: 0.1124\nEpoch 3/100\n1875/1875 [==============================] - 5s 3ms/step - loss: 2.3013 - accuracy: 0.1124\nEpoch 4/100\n1875/1875 [==============================] - 5s 3ms/step - loss: 2.3014 - accuracy: 0.1124\nEpoch 5/100\n1875/1875 [==============================] - 5s 3ms/step - loss: 2.3013 - accuracy: 0.1124\nEpoch 6/100\n1875/1875 [==============================] - 5s 3ms/step - loss: 2.3013 - accuracy: 0.1124\nEpoch 7/100\n1875/1875 [==============================] - 5s 3ms/step - loss: 2.3013 - accuracy: 0.1124\nEpoch 8/100\n1875/1875 [==============================] - 5s 3ms/step - loss: 2.3013 - accuracy: 0.1124\nEpoch 9/100\n1875/1875 [==============================] - 5s 3ms/step - loss: 2.3013 - accuracy: 0.1124\nEpoch 10/100\n1875/1875 [==============================] - 5s 3ms/step - loss: 2.3013 - accuracy: 0.1124\nEpoch 11/100\n1875/1875 [==============================] - 5s 3ms/step - loss: 2.3013 - accuracy: 0.1124\nEpoch 12/100\n1875/1875 [==============================] - 5s 3ms/step - loss: 2.3013 - accuracy: 0.1124\nEpoch 13/100\n1875/1875 [==============================] - 5s 3ms/step - loss: 2.3014 - accuracy: 0.1124\nEpoch 14/100\n1875/1875 [==============================] - 5s 3ms/step - loss: 2.3014 - accuracy: 0.1124\nEpoch 15/100\n1875/1875 [==============================] - 5s 3ms/step - loss: 2.3013 - accuracy: 0.1124\nEpoch 16/100\n1875/1875 [==============================] - 5s 3ms/step - loss: 2.3014 - accuracy: 0.1124\nEpoch 17/100\n1875/1875 [==============================] - 5s 3ms/step - loss: 2.3013 - accuracy: 0.1124\nEpoch 18/100\n1875/1875 [==============================] - 5s 3ms/step - loss: 2.3013 - accuracy: 0.1124\nEpoch 19/100\n1875/1875 [==============================] - 5s 3ms/step - loss: 2.3013 - accuracy: 0.1124\nEpoch 20/100\n1875/1875 [==============================] - 5s 3ms/step - loss: 2.3013 - accuracy: 0.1124\nEpoch 21/100\n1875/1875 [==============================] - 5s 3ms/step - loss: 2.3013 - accuracy: 0.1124\nEpoch 22/100\n1875/1875 [==============================] - 5s 3ms/step - loss: 2.3013 - accuracy: 0.1124\nEpoch 23/100\n1875/1875 [==============================] - 5s 3ms/step - loss: 2.3013 - accuracy: 0.1124\nEpoch 24/100\n1875/1875 [==============================] - 5s 3ms/step - loss: 2.3014 - accuracy: 0.1124\nEpoch 25/100\n1875/1875 [==============================] - 5s 3ms/step - loss: 2.3014 - accuracy: 0.1124\nEpoch 26/100\n1875/1875 [==============================] - 5s 3ms/step - loss: 2.3014 - accuracy: 0.1124\nEpoch 27/100\n1875/1875 [==============================] - 5s 3ms/step - loss: 2.3013 - accuracy: 0.1124\nEpoch 28/100\n1875/1875 [==============================] - 5s 3ms/step - loss: 2.3013 - accuracy: 0.1124\nEpoch 29/100\n1875/1875 [==============================] - 7s 4ms/step - loss: 2.3013 - accuracy: 0.1124\nEpoch 30/100\n1875/1875 [==============================] - 5s 3ms/step - loss: 2.3013 - accuracy: 0.1124\nEpoch 31/100\n1875/1875 [==============================] - 5s 3ms/step - loss: 2.3014 - accuracy: 0.1124\nEpoch 32/100\n1875/1875 [==============================] - 5s 3ms/step - loss: 2.3013 - accuracy: 0.1124\nEpoch 33/100\n1875/1875 [==============================] - 5s 3ms/step - loss: 2.3013 - accuracy: 0.1124\nEpoch 34/100\n1875/1875 [==============================] - 5s 3ms/step - loss: 2.3013 - accuracy: 0.1124\nEpoch 35/100\n1875/1875 [==============================] - 5s 3ms/step - loss: 2.3013 - accuracy: 0.1124\nEpoch 36/100\n1875/1875 [==============================] - 5s 3ms/step - loss: 2.3014 - accuracy: 0.1124\nEpoch 37/100\n1875/1875 [==============================] - 5s 3ms/step - loss: 2.3014 - accuracy: 0.1124\nEpoch 38/100\n1875/1875 [==============================] - 5s 3ms/step - loss: 2.3014 - accuracy: 0.1124\nEpoch 39/100\n1875/1875 [==============================] - 5s 2ms/step - loss: 2.3013 - accuracy: 0.1124\nEpoch 40/100\n1875/1875 [==============================] - 5s 3ms/step - loss: 2.3014 - accuracy: 0.1124\nEpoch 41/100\n1875/1875 [==============================] - 5s 3ms/step - loss: 2.3013 - accuracy: 0.1124\nEpoch 42/100\n1875/1875 [==============================] - 5s 3ms/step - loss: 2.3014 - accuracy: 0.1124\nEpoch 43/100\n1875/1875 [==============================] - 5s 3ms/step - loss: 2.3013 - accuracy: 0.1124\nEpoch 44/100\n1875/1875 [==============================] - 5s 3ms/step - loss: 2.3013 - accuracy: 0.1124\nEpoch 45/100\n1875/1875 [==============================] - 5s 3ms/step - loss: 2.3013 - accuracy: 0.1124\nEpoch 46/100\n1875/1875 [==============================] - 5s 3ms/step - loss: 2.3014 - accuracy: 0.1124\nEpoch 47/100\n1875/1875 [==============================] - 5s 3ms/step - loss: 2.3014 - accuracy: 0.1124\nEpoch 48/100\n1875/1875 [==============================] - 5s 3ms/step - loss: 2.3013 - accuracy: 0.1124\nEpoch 49/100\n1875/1875 [==============================] - 5s 3ms/step - loss: 2.3013 - accuracy: 0.1124\nEpoch 50/100\n1875/1875 [==============================] - 5s 3ms/step - loss: 2.3014 - accuracy: 0.1124\nEpoch 51/100\n1875/1875 [==============================] - 5s 3ms/step - loss: 2.3014 - accuracy: 0.1124\nEpoch 52/100\n1875/1875 [==============================] - 5s 3ms/step - loss: 2.3013 - accuracy: 0.1124\nEpoch 53/100\n1875/1875 [==============================] - 5s 3ms/step - loss: 2.3014 - accuracy: 0.1124\nEpoch 54/100\n1875/1875 [==============================] - 5s 3ms/step - loss: 2.3014 - accuracy: 0.1124\nEpoch 55/100\n1875/1875 [==============================] - 5s 3ms/step - loss: 2.3013 - accuracy: 0.1124\nEpoch 56/100\n1875/1875 [==============================] - 5s 3ms/step - loss: 2.3013 - accuracy: 0.1124\nEpoch 57/100\n1875/1875 [==============================] - 5s 3ms/step - loss: 2.3014 - accuracy: 0.1124\nEpoch 58/100\n1532/1875 [=======================>......] - ETA: 0s - loss: 2.3013 - accuracy: 0.1120\n\n\n\n#evaluating our network against the data , since we had earlier mentioned metric as accurracy , the ouput will provide us with accuracy of our model\n\ntest_loss ,test_acc = network.evaluate(test_images,test_labels)\nprint(\"test accuracy:\",test_acc)\n\n313/313 [==============================] - 0s 1ms/step - loss: 2.3011 - accuracy: 0.1135\ntest accuracy: 0.11349999904632568"
  },
  {
    "objectID": "docu/Book_Implementations/Untitled.html",
    "href": "docu/Book_Implementations/Untitled.html",
    "title": "Krishnakant A",
    "section": "",
    "text": "import sys\nprint(sys.executable)\nprint(sys.version)\nprint(sys.version_info)\n\n/Users/krishnakantammanamanachi/opt/anaconda3/envs/tf/bin/python\n3.10.9 | packaged by conda-forge | (main, Feb  2 2023, 20:24:27) [Clang 14.0.6 ]\nsys.version_info(major=3, minor=10, micro=9, releaselevel='final', serial=0)\n\n\n\nfrom keras.datasets import mnist\n[(train_images,train_labels),(test_images,test_labels)] = mnist.load_data()\n\n\nprint( \"Shape of train images\",train_images.shape )\nprint( \"Shape of test images\",test_images.shape )\nprint( \"Train labels format\",train_labels )\n\nShape of train images (60000, 28, 28)\nShape of test images (10000, 28, 28)\nTrain labels format [5 0 4 ... 5 6 8]\n\n\n\n\n\nfrom keras import models , layers\n# Creating a two layer dense(all hidden nodes are connected all input nodes - fully connected) neural network.\n# activation function for each layer is specified\n# \nnetwork = models.Sequential()\nnetwork.add(layers.Dense(512,activation = 'relu',input_shape = (28*28,))) # Creating a neural network layer with 512 hidden nodes and 'relu' as activation function\nnetwork.add(layers.Dense(10,activation = 'softmax')) # creating another layer in succession to the previous layer with 10 nodes as output \n\n\n## The Compiler Step\n\nnetwork.compile(optimizer = 'rmsprop',\n                 loss = 'categorical_crossentropy',\n                  metrics = ['accuracy'])\n\n\n## Preparing the image data\n\ntrain_images = train_images.reshape((60000,28*28)) # a neural network can only take a 1-d vector as an input \ntrain_images = train_images.astype('float32')/255 # Changing the type of the input data and scaling it such that the value lies between [0,1]\n\ntest_images = test_images.reshape((10000,28*28))\ntest_images = test_images.astype('float32')/255\n\ntrain_images.shape\n\n(60000, 784)\n\n\n\n## preparing the labels \n# since our labels are categorical in nature while a neural netowrk can only understand vector representations we convert our prediction labels into\n# required format\n\nfrom keras.utils import to_categorical\n\ntrain_labels = to_categorical(train_labels)\ntest_labels  = to_categorical(test_labels)\n\n\nimport sys\nimport tensorflow.keras\nimport pandas as pd\n#import sklearn as sk\nimport scipy as sp\nimport tensorflow as tf\nimport platform\nprint(f\"Python Platform: {platform.platform()}\")\nprint(f\"Tensor Flow Version: {tf.__version__}\")\nprint(f\"Keras Version: {tensorflow.keras.__version__}\")\nprint()\nprint(f\"Python {sys.version}\")\nprint(f\"Pandas {pd.__version__}\")\n#print(f\"Scikit-Learn {sk.__version__}\")\nprint(f\"SciPy {sp.__version__}\")\ngpu = len(tf.config.list_physical_devices('GPU'))>0\nprint(\"GPU is\", \"available\" if gpu else \"NOT AVAILABLE\")\n\nPython Platform: macOS-13.1-x86_64-i386-64bit\nTensor Flow Version: 2.11.0\nKeras Version: 2.11.0\n\nPython 3.10.9 | packaged by conda-forge | (main, Feb  2 2023, 20:24:27) [Clang 14.0.6 ]\nPandas 1.5.3\nSciPy 1.10.0\nGPU is NOT AVAILABLE\n\n\n\n## Passing our input and output data to the neural network we have designed \nnetwork.fit(train_images,train_labels,epochs = 100)\n\nEpoch 1/100\n1875/1875 [==============================] - 5s 3ms/step - loss: 2.3013 - accuracy: 0.1124\nEpoch 2/100\n1875/1875 [==============================] - 5s 3ms/step - loss: 2.3013 - accuracy: 0.1124\nEpoch 3/100\n1875/1875 [==============================] - 5s 3ms/step - loss: 2.3013 - accuracy: 0.1124\nEpoch 4/100\n1875/1875 [==============================] - 5s 3ms/step - loss: 2.3014 - accuracy: 0.1124\nEpoch 5/100\n1875/1875 [==============================] - 5s 3ms/step - loss: 2.3013 - accuracy: 0.1124\nEpoch 6/100\n 139/1875 [=>............................] - ETA: 4s - loss: 2.3008 - accuracy: 0.1093\n\n\n\n#evaluating our network against the data , since we had earlier mentioned metric as accurracy , the ouput will provide us with accuracy of our model\n\ntest_loss ,test_acc = network.evaluate(test_images,test_labels)\nprint(\"test accuracy:\",test_acc)\n\n313/313 [==============================] - 0s 1ms/step - loss: 2.3011 - accuracy: 0.1135\ntest accuracy: 0.11349999904632568"
  }
]