{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: 'Hands-On Machine Learning with Scikit-Learn ,Keras and Tensorflow'\n",
        "execute:\n",
        "  warning: false\n",
        "  freeze: true\n",
        "  keep-ipynb: true\n",
        "toc: true\n",
        "format:\n",
        "  html:\n",
        "    html-math-method: katex\n",
        "    code-block-bg: '#E5E7E9'\n",
        "    df-print: paged\n",
        "    code-overflow: wrap\n",
        "    code-copy: true\n",
        "---"
      ],
      "id": "b4f508d3"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Image of the book](IMG_9683.jpg)\n",
        "\n",
        "## Introduction\n",
        "\n",
        "A pure code based implementation with necessary mathematical formulations included.\n",
        "\n",
        "### End to End machine Learning\n",
        "\n",
        "### Performance Measure\n",
        "\n",
        "<b> <u>Root Mean Square Error </b> </u> $$ RMSE(\\textbf{X},h)= \\sqrt{\\frac{\\sum_{i = 1}^{m}{(h{(x^{(i)})- y ^{(i)}}})^2}{m}}$$ $m$- is the number of instances in the datasets you are measuring the RMSE on <br /> $x^{(i)}$- is the vector of all teh feature values of the $i^{th}$ instance in the dataset<br/> $y^{(i)}$- is its label<br/> $\\textbf{X}$- is the matrix containing all the feature value of all the instances in the dataset. $x^{(i)}$ transpose is the $i^{th}$-row in $\\textbf{X}$ </br> h - is the system's prediction function also called as hypothesis. It is also represented by $\\hat{y}$ where $\\hat{y}^{(i)} = h(x^{(i)})$</br> $RMSE(\\textbf{X},h)$- is the cost function measured on the set of examples using the hypothesis h </br>\n",
        "\n",
        "<b> <u>Mean absolute Error</b> </u > $$ MAE(\\textbf{X},h)= \\frac{\\sum_{i = 1}^{m}{|h{(x^{(i)})- y ^{(i)}}}|}{m}$$\n",
        "\n",
        "The difference between these two performance measure is how the norm is caculated. RSME makes use of $\\mathcal{l}_2$ norm while MAE makes use of $\\mathcal{l}_1$ norm.\n",
        "\n",
        "#### Download the DATA\n"
      ],
      "id": "e6e37e6f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os \n",
        "import tarfile\n",
        "import urllib\n",
        "\n",
        "Download_Root = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\n",
        "Housing_Path = os.path.join(\"Data\",\"housing\") # for creating your local directory with DATA as main directory and housign as sub directory\n",
        "Housing_URL = Download_Root + \"datasets/housing/housing.tgz\"\n",
        "\n",
        "def fetch_housing_data(housing_url = Housing_URL,housing_path = Housing_Path):\n",
        "    os.makedirs(housing_path,exist_ok = True)# Will create a directory with name at housing path , exist_ok will check if it already exist , if yes leaving it unaltered\n",
        "    tgz_path = os.path.join(housing_path,\"housing.tgz\")\n",
        "    urllib.request.urlretrieve(housing_url,tgz_path)\n",
        "    housing_tgz = tarfile.open(tgz_path)\n",
        "    housing_tgz.extractall(path=housing_path)\n",
        "    housing_tgz.close()\n",
        "    \n",
        "fetch_housing_data() # call the function"
      ],
      "id": "ffc87b20",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "\n",
        "def load_housing_data(housing_path = Housing_Path):\n",
        "    csv_path = os.path.join(housing_path,\"housing.csv\")\n",
        "    return pd.read_csv(csv_path)"
      ],
      "id": "c5b1f26f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "housing = load_housing_data()\n",
        "housing.head()"
      ],
      "id": "e25ff3c3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "housing.info()"
      ],
      "id": "33f89200",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "housing[\"ocean_proximity\"].value_counts()"
      ],
      "id": "26e99f8a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "housing.describe()"
      ],
      "id": "cfdb975e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%matplotlib inline \n",
        "import matplotlib.pyplot as plt\n",
        "housing.hist(bins = 50 ,figsize = (20,15))"
      ],
      "id": "8d226a06",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Few point about the data\n",
        "\n",
        "-   The median income is capped at 15 for higher median incomes and at 0.5 for lower median incomes. The numbers are denominated in \\$10000 i.e 3 in the chart represents 30,000 USD\n",
        "\n",
        "-   The housing median age and median house value were also capped. Since median house value is our target variable this is a source of a problem.The prediction might never go beyond the capped limit.\n",
        "\n",
        "-   The attributes have different scales\n",
        "\n",
        "-   Many of the histograms are tail-heavy. Important to normalize the data as some machine learning algorithims might fail to detect patterns effectively\n",
        "\n",
        "#### Creating a test set and train set\n"
      ],
      "id": "a8d70578"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "np.random.seed(50)# will always give the same test set and train set \n",
        "def split_train_test(data,test_ratio):\n",
        "    shuffled_indices = np.random.permutation(len(data))\n",
        "    test_set_size = int(len(data)*test_ratio)\n",
        "    test_indices = shuffled_indices[:test_set_size]\n",
        "    train_indices = shuffled_indices[test_set_size:]\n",
        "    return data.loc[train_indices] ,data.loc[test_indices]\n",
        "\n",
        "train_set , test_set = split_train_test(housing,0.2)\n",
        "print(len(train_set))\n",
        "print(len(test_set))"
      ],
      "id": "6594b16a",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}